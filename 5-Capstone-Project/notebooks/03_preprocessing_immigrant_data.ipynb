{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from itertools import chain\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAFOLDER = '/Users/christian/Data/udacity_capstone/'\n",
    "OUTPUTFOLDER = '/Users/christian/Data/udacity_capstone/output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_immi_schema():\n",
    "    \"\"\"\n",
    "    Map the column names to datatypes and return as a schema.\n",
    "    \"\"\"\n",
    "    immi_schema = T.StructType([\n",
    "        T.StructField('cicid', T.IntegerType()),\n",
    "        T.StructField('i94yr', T.StringType()),\n",
    "        T.StructField('i94mon', T.StringType()),\n",
    "        T.StructField('i94cit', T.StringType()),\n",
    "        T.StructField('i94res', T.IntegerType()),\n",
    "        T.StructField('i94port', T.StringType()),\n",
    "        T.StructField('arrdate', T.StringType()),\n",
    "        T.StructField('i94mode', T.StringType()),\n",
    "        T.StructField('i94addr', T.StringType()),\n",
    "        T.StructField('depdate', T.StringType()),\n",
    "        T.StructField('i94bir', T.StringType()),\n",
    "        T.StructField('i94visa', T.StringType()),\n",
    "        T.StructField('count',  T.IntegerType()),\n",
    "        T.StructField('dtadfile', T.StringType()),\n",
    "        T.StructField('visapost', T.IntegerType()),\n",
    "        T.StructField('occup', T.StringType()),\n",
    "        T.StructField('entdepa', T.StringType()),\n",
    "        T.StructField('entdepd', T.IntegerType()),\n",
    "        T.StructField('entdepu', T.StringType()),\n",
    "        T.StructField('matflag', T.StringType()),\n",
    "        T.StructField('biryear', T.StringType()),\n",
    "        T.StructField('dtaddto', T.StringType()),\n",
    "        T.StructField('gender', T.StringType()),\n",
    "        T.StructField('insnum', T.StringType()),\n",
    "        T.StructField('airline', T.StringType()),\n",
    "        T.StructField('admnum', T.StringType()),\n",
    "        T.StructField('fltno', T.StringType()),\n",
    "        T.StructField('visatype', T.StringType()),\n",
    "    ])\n",
    "    return immi_schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = get_immi_schema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "#df_spark = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "df_spark = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(DATAFOLDER, 'sas_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "|   cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|         admnum|fltno|visatype|\n",
      "+--------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "|459651.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|     FL|20559.0|  54.0|    2.0|  1.0|20160403|    null| null|      O|      R|   null|      M| 1962.0|07012016|  null|  null|     VS|5.5556253633E10|00115|      WT|\n",
      "|459652.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|     FL|20555.0|  74.0|    2.0|  1.0|20160403|    null| null|      T|      O|   null|      M| 1942.0|07012016|     F|  null|     VS|   6.74406485E8|  103|      WT|\n",
      "|459653.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|     FL|20557.0|  44.0|    2.0|  1.0|20160403|    null| null|      T|      Q|   null|      M| 1972.0|10022016|     M|  null|     VS|   6.74948185E8|  109|      B2|\n",
      "|459654.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|      G|20555.0|  38.0|    2.0|  1.0|20160403|    null| null|      O|      O|   null|      M| 1978.0|07012016|  null|  null|     VS|5.5541762033E10|00103|      WT|\n",
      "|459655.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|     GA|   null|  64.0|    2.0|  1.0|20160403|    null| null|      G|   null|   null|   null| 1952.0|07012016|     F|  null|     VS|5.5541328433E10|00103|      WT|\n",
      "|459656.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|     GA|   null|  63.0|    2.0|  1.0|20160403|    null| null|      G|   null|   null|   null| 1953.0|07012016|     M|  null|     BA|5.5578035433E10|00227|      WT|\n",
      "|459657.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|     GA|20548.0|  44.0|    1.0|  1.0|20160403|    null| null|      G|      I|   null|      M| 1972.0|07012016|     M|  null|     BA|5.5578919033E10|00227|      WB|\n",
      "|459658.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|     GA|20548.0|  39.0|    1.0|  1.0|20160403|    null| null|      G|      I|   null|      M| 1977.0|07012016|     M|  null|     BA|5.5578743033E10|00227|      WB|\n",
      "|459659.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|     GA|20573.0|  84.0|    2.0|  1.0|20160403|    null| null|      G|      I|   null|      M| 1932.0|07012016|     M|  null|     BA|5.5577459833E10|00227|      WT|\n",
      "|459660.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|     GA|20555.0|  55.0|    2.0|  1.0|20160403|    null| null|      G|      N|   null|      M| 1961.0|07012016|     M|  null|     BA|5.5577874033E10|00227|      WT|\n",
      "|459661.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|     GA|20555.0|  54.0|    2.0|  1.0|20160403|    null| null|      G|      N|   null|      M| 1962.0|07012016|     M|  null|     BA|5.5577725633E10|00227|      WT|\n",
      "|459662.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|     GA|20548.0|  36.0|    2.0|  1.0|20160403|    null| null|      G|      O|   null|      M| 1980.0|07012016|     M|  null|     DL|5.5577644033E10|00349|      WT|\n",
      "|459663.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|     GA|20549.0|  71.0|    1.0|  1.0|20160403|    null| null|      G|      O|   null|      M| 1945.0|07012016|     M|  null|     DL|5.5574698833E10|00031|      WB|\n",
      "|459664.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|     GA|20549.0|  39.0|    2.0|  1.0|20160403|    null| null|      G|      O|   null|      M| 1977.0|07012016|     M|  null|     VS|5.5560145833E10|00109|      WT|\n",
      "|459665.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|     GA|20550.0|  54.0|    1.0|  1.0|20160403|    null| null|      G|      O|   null|      M| 1962.0|07012016|     M|  null|     BA|5.5577579933E10|00227|      WB|\n",
      "|459666.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|     GA|20550.0|  51.0|    1.0|  1.0|20160403|    null| null|      G|      O|   null|      M| 1965.0|07012016|     M|  null|     BA|5.5578365433E10|00227|      WB|\n",
      "|459667.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|     GA|20550.0|  43.0|    1.0|  1.0|20160403|    null| null|      G|      O|   null|      M| 1973.0|07012016|     M|  null|     DL|5.5574951833E10|00031|      WB|\n",
      "|459668.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|     GA|20550.0|  37.0|    1.0|  1.0|20160403|    null| null|      G|      O|   null|      M| 1979.0|07012016|     F|  null|     DL|5.5564568733E10|00029|      WB|\n",
      "|459669.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|     GA|20550.0|  33.0|    1.0|  1.0|20160403|    null| null|      G|      O|   null|      M| 1983.0|07012016|     F|  null|     VS|5.5557739533E10|00109|      WB|\n",
      "|459670.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|     GA|20550.0|  50.0|    1.0|  1.0|20160403|    null| null|      G|      O|   null|      M| 1966.0|07012016|     M|  null|     DL|5.5574648333E10|00031|      WB|\n",
      "+--------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check uniqueness of admission number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2641028"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2641028"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.select('cicid').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2641028"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.drop_duplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = df_spark.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2641028"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.count() == df_spark.select('admnum').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2637526"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.select('admnum').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_admnum = df_spark.groupBy('admnum').count().filter('count > 2').orderBy(F.desc('count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89977239030.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_admnum.first()['admnum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "| 155049.0|2016.0|   4.0| 582.0| 582.0|    BRO|20545.0|    1.0|     TX|20552.0|  32.0|    1.0|  1.0|20160401|     MER| null|      H|      R|   null|      M| 1984.0|09062016|     M|  null|    *GA|8.997723903E10|N934C|      B1|\n",
      "|3241942.0|2016.0|   4.0| 582.0| 582.0|    BRO|20561.0|    1.0|     TX|20567.0|  32.0|    1.0|  1.0|20160417|     MER| null|      H|      R|   null|      M| 1984.0|09062016|     M|  null|    *GA|8.997723903E10|N934C|      B1|\n",
      "|2433523.0|2016.0|   4.0| 582.0| 582.0|    BRO|20557.0|    1.0|     TX|20560.0|  32.0|    1.0|  1.0|20160413|     MER| null|      H|      R|   null|      M| 1984.0|09062016|     M|  null|    *GA|8.997723903E10|N934C|      B1|\n",
      "|3015990.0|2016.0|   4.0| 582.0| 582.0|    BRO|20560.0|    1.0|     TX|20561.0|  32.0|    1.0|  1.0|20160416|     MER| null|      H|      R|   null|      M| 1984.0|09062016|     M|  null|    *GA|8.997723903E10|N934C|      B1|\n",
      "|1918519.0|2016.0|   4.0| 582.0| 582.0|    SPE|20554.0|    1.0|     TX|20557.0|  32.0|    1.0|  1.0|20160410|     MER| null|      H|      R|   null|      M| 1984.0|09062016|     M|  null|    *GA|8.997723903E10|N934C|      B1|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.filter('admnum == 89977239030').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = df_spark.withColumn('i94yr', df_spark['i94yr'].cast(T.IntegerType())).\\\n",
    "        withColumn('i94mon', df_spark['i94mon'].cast(T.IntegerType())).\\\n",
    "        withColumn('i94cit', df_spark['i94cit'].cast(T.IntegerType())).\\\n",
    "        withColumn('i94res', df_spark['i94res'].cast(T.IntegerType())).\\\n",
    "        withColumn('arrdate', df_spark['arrdate'].cast(T.IntegerType())).\\\n",
    "        withColumn('i94mode', df_spark['i94mode'].cast(T.IntegerType())).\\\n",
    "        withColumn('depdate', df_spark['depdate'].cast(T.IntegerType())).\\\n",
    "        withColumn('i94bir', df_spark['i94bir'].cast(T.IntegerType())).\\\n",
    "        withColumn('i94visa', df_spark['i94visa'].cast(T.IntegerType())).\\\n",
    "        withColumn('count', df_spark['count'].cast(T.IntegerType())).\\\n",
    "        withColumn('biryear', df_spark['biryear'].cast(T.IntegerType())).\\\n",
    "        withColumn('admnum', df_spark['admnum'].cast(T.IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = pd.read_csv(os.path.join(DATAFOLDER, 'immigration_data_sample.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time = df_spark\n",
    "df_time = df_time.withColumn(\"arrival_date\", F.expr(\"date_add(to_date('1960-01-01'), arrdate)\"))\n",
    "df_time = df_time.withColumn(\"depart_date\", F.expr(\"date_add(to_date('1960-01-01'), depdate)\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time.withColumn(\"diff_days\", F.datediff(\"depart_date\", \"arrival_date\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## map i94addrl - state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def map_col(df, map_col_name, df_col_name, new_col_name):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : spark dataframe\n",
    "        The file containing the df_col_name to be used for mapping.\n",
    "    map_col_name : str\n",
    "        The column name of the mapping file.\n",
    "    df_col_name : str\n",
    "        The column name in the Spark dataframe to be used.\n",
    "    new_col_name : str\n",
    "        New column name of the mapping results.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    df_map = pd.read_csv(os.path.join(DATAFOLDER, f'{map_col_name}.csv'), quotechar=\"'\")\n",
    "    id_col = f'{map_col_name}_id'\n",
    "    dic_map = dict(zip(df_map[id_col], df_map[map_col_name]))\n",
    "    mapping_expr = F.create_map([F.lit(x) for x in chain(*dic_map.items())])\n",
    "    return df.withColumn(new_col_name, mapping_expr[F.col(df_col_name)])\n",
    "\n",
    "\n",
    "\n",
    "@udf\n",
    "def udf_city_name(city_full):\n",
    "    splt = str.split(city_full, ',')\n",
    "    if len(splt) == 0:\n",
    "        return ''\n",
    "    return splt[0].capitalize().strip()\n",
    "\n",
    "@udf\n",
    "def udf_state_short(city_full):\n",
    "    splt = str.split(city_full, ',')\n",
    "    if len(splt) < 2:\n",
    "        return ''\n",
    "    return splt[1].strip()\n",
    "\n",
    "@udf\n",
    "def udf_state_format(port_state):\n",
    "    return port_state.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.select(\"i94addr\").distinct().show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO replace mapping in i94addrl to have names in captialize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df, map_col_name, df_col_name, new_col_name\n",
    "df_test = df_time\n",
    "#df_test = map_col(df_test, 'i94prtl', 'i94port', 'port')\n",
    "\n",
    "# city name of arrival port\n",
    "#df_test = df_test.withColumn(\"port_city\", udf_city_name(\"port\"))\n",
    "df_test = map_col(df_test, 'prtl_city', 'i94port', 'port_city')\n",
    "#df_test = df_test.withColumn(\"port_state_short\", udf_state_short(\"port\"))\n",
    "df_test = map_col(df_test, 'prtl_state', 'i94port', 'port_state_short')\n",
    "df_test = map_col(df_test, 'addrl', 'port_state_short', 'port_state')\n",
    "#df_test = df_test.withColumn('port_state_2', udf_state_format('port_state'))\n",
    "\n",
    "# country of entry, country of citizenship\n",
    "df_test = map_col(df_test, 'cntyl', 'i94port', 'state_cit')\n",
    "df_test = map_col(df_test, 'cntyl', 'i94port', 'state_res')\n",
    "#df_test = map_col(df_test, 'i94addrl', 'i94addr', 'port_state')\n",
    "df_test = map_col(df_test, 'visa', 'i94visa', 'visa')\n",
    "#df_test = df_test.withColumn(\"port_state\", udf_state_format(\"port_state\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is arrdate ever null?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.where(F.col(\"arrdate\").isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge with demographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#demographic = pd.read_csv(os.path.join(DATAFOLDER, 'us-cities-demographics.csv'), sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics = spark.read.format('csv').options(header='true', sep=';', inferSchema=True).\\\n",
    "    load(os.path.join(DATAFOLDER, 'us-cities-demographics.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "match airport with demographic table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up unique combinations of city and state in fact table\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset with unique combinations of city, state and \n",
    "# port_city, port_state, i94port (as key )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_read_csv(spark, folder, filename, **kwargs):\n",
    "\n",
    "    return spark.read.format('csv').options(header='true', inferSchema=True, **kwargs).\\\n",
    "        load(os.path.join(folder, filename))\n",
    "\n",
    "def format_column_names(s):\n",
    "    s = s.casefold()\n",
    "    s = s.replace(' ', '_')\n",
    "    s = s.replace('-', '_')\n",
    "    return s\n",
    "\n",
    "def rename_columns(df):\n",
    "    old_names = df.schema.names\n",
    "    new_names = [format_column_names(s) for s in old_names]\n",
    "    df = reduce(lambda df, idx: \n",
    "                df.withColumnRenamed(old_names[idx], new_names[idx]), range(len(old_names)), df)\n",
    "    return df\n",
    "\n",
    "def create_demographic_table(spark, datafolder, outputfolder): \n",
    "    # select a subset of original table\n",
    "    #df_demo = df.select(['i94port']).dropDuplicates()\n",
    "    df_demo = spark_read_csv(spark, datafolder, 'prtl_city.csv')\n",
    "    #pd.read_csv(os.path.join(datafolder, 'prtl.csv'), quotechar=\"'\")\n",
    "    demographics = spark_read_csv(spark, datafolder, 'us-cities-demographics.csv', sep=';')\n",
    "    #pd.read_csv(os.path.join(datafolder, 'us-cities-demographics.csv'), quotechar=\"'\")\n",
    "    \n",
    "    # do the preprocessing, append columns\n",
    "    #df_demo = map_col(df_demo, 'prtl_city', 'prtl_city', 'port_city')\n",
    "    df_demo = df_demo.withColumnRenamed('prtl_city', 'port_city')\n",
    "    #df_test = df_test.withColumn(\"port_state_short\", udf_state_short(\"port\"))\n",
    "    df_demo = map_col(df_demo, 'prtl_state', 'prtl_city_id', 'port_state_short')\n",
    "    df_demo = map_col(df_demo, 'addrl', 'port_state_short', 'port_state')\n",
    "    \n",
    "    # we are deleting the ones, for which no states were found, since we only focus on the US here\n",
    "    # later expand world wide\n",
    "    df_demo = df_demo.dropna(subset=['port_state'])\n",
    "    df_demo = df_demo.join(demographics, (df_demo.port_city == demographics.City) & (df_demo.port_state_short == demographics['State Code']))\n",
    "    columns_to_drop = ['port_city', 'port_state', 'port_state_short']\n",
    "    df_demo = df_demo.drop(*columns_to_drop)\n",
    "    \n",
    "    # reformat column names\n",
    "    df_demo = rename_columns(df_demo)\n",
    "    df_demo.write.parquet(os.path.join(outputfolder, 'city_demographics.parquet'), 'overwrite')\n",
    "    return df_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo = create_demographic_table(spark, DATAFOLDER, OUTPUTFOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create single tables for everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## country table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_read_csv(folder, filename, **kwargs):\n",
    "\n",
    "    return spark.read.format('csv').options(header='true', inferSchema=True, **kwargs).\\\n",
    "        load(os.path.join(folder, filename))\n",
    "\n",
    "def csv_to_parquet(datafolder, outputfolder, csv_name, table_name): \n",
    "    df = spark_read_csv(datafolder, f'{csv_name}.csv', sep=',', quotechar=\"'\")\n",
    "    df = df.withColumnRenamed('value', 'id')\n",
    "    df.write.parquet(os.path.join(outputfolder, f'{table_name}.parquet'), 'overwrite')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntyl = csv_to_parquet(DATAFOLDER, OUTPUTFOLDER, 'i94cntyl', 'country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_col_name = 'prtl_state'\n",
    "df_map = spark_read_csv(spark, datafolder, f'{map_col_name}.csv')\n",
    "#pd.read_csv(os.path.join(DATAFOLDER, f'{map_col_name}.csv'), quotechar=\"'\")\n",
    "df_map = df_map.toPandas()\n",
    "id_col = f'{map_col_name}_id'\n",
    "dic_map = dict(zip(df_map[id_col], df_map[map_col_name]))\n",
    "mapping_expr = F.create_map([F.lit(x) for x in chain(*dic_map.items())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_map[id_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model table i94model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transport = csv_to_parquet(DATAFOLDER, OUTPUTFOLDER, 'i94model', 'transport')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transport.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visa table i94visa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visa = csv_to_parquet(DATAFOLDER, OUTPUTFOLDER, 'i94visa', 'visa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visa.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## global annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "# also aggregate by year\n",
    "folder = os.path.join(DATAFOLDER, 'climate-change-earth-surface-temperature-data')\n",
    "global_annual = csv_to_parquet(folder, OUTPUTFOLDER, 'GlobalTemperatures', 'visa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_full_time_table(outputfolder, daysafter=36525):\n",
    "\n",
    "    days_till_2060 = range(daysafter)\n",
    "    all_dates = [(t,) for t in days_till_2060]\n",
    "    \n",
    "    t_schema = T.StructType([T.StructField('i94_date', T.IntegerType())])\n",
    "    timeframe = spark.createDataFrame(all_dates, t_schema)\n",
    "    timeframe = timeframe.withColumn(\"dt_date\", F.expr(\"date_add(to_date('1960-01-01'), i94_date)\"))\n",
    "    timeframe = timeframe.select('i94_date', 'dt_date',\n",
    "                    F.year('dt_date').alias('year'),\n",
    "                    F.month('dt_date').alias('month'),\n",
    "                    F.dayofmonth('dt_date').alias('day'),\n",
    "                    F.dayofweek('dt_date').alias('weekday'))\n",
    "    \n",
    "    timeframe.write.partitionBy('year', 'month').parquet(os.path.join(outputfolder, 'dates.parquet'), 'overwrite')\n",
    "    return timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeframe = create_full_time_table(OUTPUTFOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeframe.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeframe.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_full_time_table(outputfolder, daysafter=36525):\n",
    "\n",
    "    future_days = range(daysafter)\n",
    "    all_dates = [(t,) for t in future_days]\n",
    "    \n",
    "    t_schema = T.StructType([T.StructField('i_date', T.IntegerType())])\n",
    "    timeframe = spark.createDataFrame(all_dates, t_schema)\n",
    "    timeframe = timeframe.withColumn(\"dt_date\", F.expr(\"date_add(to_date('1960-01-01'), i_date)\"))\n",
    "    timeframe = timeframe.select('i_date', 'dt_date',\n",
    "                    F.year('dt_date').alias('year'),\n",
    "                    F.month('dt_date').alias('month'),\n",
    "                    F.dayofmonth('dt_date').alias('day'),\n",
    "                    F.dayofweek('dt_date').alias('weekday'))\n",
    "    \n",
    "    timeframe.write.partitionBy('year', 'month').parquet(os.path.join(outputfolder, 'dates.parquet'), 'overwrite')\n",
    "    return timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = create_full_time_table(OUTPUTFOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO apply rolling window function on average of last 10 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = os.path.join(DATAFOLDER, 'climate-change-earth-surface-temperature-data')\n",
    "filename = 'GlobalLandTemperaturesByCountry.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate = spark_read_csv(folder, filename, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert timestamp to date\n",
    "already done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate = climate.withColumn('dt', F.to_date(F.col('dt')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge with country id from country table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate.agg({\"dt\": \"min\"}).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate.agg({\"dt\": \"max\"}).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(OUTPUTFOLDER, 'country.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate = climate.join(country, on=country['i94cntyl'] == climate['Country'], how='leftouter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate = climate.drop('i94cntyl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_climate_country(folder, filename, outputfolder):\n",
    "    climate = spark_read_csv(folder, filename, sep=',')\n",
    "    climate = climate.withColumn('dt', F.to_date(F.col('dt')))\n",
    "    country = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(outputfolder, 'country.parquet'))\n",
    "    climate = climate.join(country, on=country['i94cntyl'] == climate['Country'], how='leftouter')\n",
    "    climate = climate.drop('i94cntyl')\n",
    "    climate = rename_columns(climate)\n",
    "    climate = climate.withColumn('year', F.year('dt').alias('year'))\n",
    "    climate = climate.withColumnRenamed('id', 'country_id').\\\n",
    "        withColumnRenamed('averagetemperatureuncertainty', 'avg_uncertainty').\\\n",
    "        withColumnRenamed('averagetemperature', 'avg_temperature')\n",
    "    climate.write.partitionBy('year', 'country').parquet(os.path.join(outputfolder, 'climate_country.parquet'), 'overwrite')\n",
    "    return climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = os.path.join(DATAFOLDER, 'climate-change-earth-surface-temperature-data')\n",
    "filename = 'GlobalLandTemperaturesByCountry.csv'\n",
    "climate_country = generate_climate_country(folder, filename, OUTPUTFOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_country = climate_country.withColumnRenamed('id', 'country_id').\\\n",
    "    withColumnRenamed('averagetemperatureuncertainty', 'avg_uncertainty')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aggregate annual average temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual = climate.withColumn('year', F.year('dt').alias('year'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_country.groupby([F.col('country'), F.col('country_id'), F.col('year')]).agg(F.avg('averagetemperature').alias('avg_temperature')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(OUTPUTFOLDER, 'climate_country.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_annual_temp_table(outputfolder):\n",
    "    annual = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(OUTPUTFOLDER, 'climate_country.parquet'))\n",
    "    annual.groupby([F.col('country'), F.col('country_id'), F.col('year')]).agg(F.avg('avg_temperature').alias('avg_temperature'))\n",
    "    annual.write.partitionBy('country').parquet(os.path.join(outputfolder, 'annual_climate_country.parquet'), 'overwrite')\n",
    "    return annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(OUTPUTFOLDER, 'climate_country.parquet'))\n",
    "annual.groupby([F.col('country'), F.col('country_id'), F.col('year')]).agg(F.avg('averagetemperature').alias('avg_temperature'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo when storing partition by country and year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asylum Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/dhs/refugee-report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fact Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate length of stay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeframe.filter(\"i94_date = 20573\").select('dt_date').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact = df_test.withColumn(\"arrival_dt\", F.expr(\"date_add(to_date('1960-01-01'), arrdate)\"))\n",
    "df_fact = df_fact.withColumn(\"depart_dt\", F.expr(\"date_add(to_date('1960-01-01'), depdate)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_columns = ['cicid', 'i94yr', 'i94mon', 'arrival_dt', 'arrdate', 'depdate', 'i94cit', 'i94res', 'i94port', 'i94mode', 'i94addr', 'i94bir',\n",
    "               'i94visa', 'visatype', 'biryear', 'gender', 'airline', 'fltno', 'length_stay']\n",
    "\n",
    "#maybe = ['entdepa', 'entdepd', 'matflag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact = df_fact.withColumn(\"length_stay\", F.datediff(\"depart_date\", \"arrival_date\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact.select(keep_columns).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_facts_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test.join(timeframe, on=df_test['arrdate']==timeframe['i94_date'], how='leftouter').show() \n",
    "# too complicated\n",
    "#df_test.join(timeframe, on=df_test['arrdate']==timeframe['i94_date'], how='left').dropDuplicates().select(timeframe['dt_date']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.withColumn(\"length_stay\", F.datediff(\"depart_date\", \"arrival_date\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# TODO\n",
    "\n",
    "- copy my csv files to S3 manually\n",
    "with a DAG\n",
    "- write some csvs to redshift \n",
    "- copy the stuff to redshift\n",
    "- preprocessed tables as parquet files?\n",
    "- maybe split it up into two DAGs\n",
    "-- one does the intialization of the dimension tables, which won't change that often\n",
    "-- the other one does the updating of the fact table\n",
    "\n",
    "- add IATA code to city ?\n",
    "\n",
    "- or maybe the nodes can be confiured to only run once?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. csv von S3 -> redshift\n",
    "2. parquet von S3 -> Spark -> redshift, maybe like this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://sonra.io/2018/01/01/using-apache-airflow-to-build-a-data-pipeline-on-aws/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COPY listing\n",
    "FROM 's3://mybucket/data/listings/parquet/'\n",
    "IAM_ROLE 'arn:aws:iam::0123456789012:role/MyRedshiftRole'\n",
    "FORMAT AS PARQUET;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If nothing works use Docker\n",
    "https://towardsdatascience.com/getting-started-with-airflow-using-docker-cd8b44dbff98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional databases\n",
    "- https://www.kaggle.com/open-flights/flight-route-database\n",
    "- https://www.kaggle.com/dhs/refugee-report\n",
    "- https://www.dhs.gov/immigration-statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUTFOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviewing all the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(OUTPUTFOLDER, 'country.parquet')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## temperature_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_country = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(OUTPUTFOLDER, 'temperature_country.parquet')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_country.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_country.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_country.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## temperature_annual_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_country = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(OUTPUTFOLDER, 'temperature_annual_country.parquet')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cntyl_id', 'year', 'average_temperature', 'country']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp_country.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cntyl_id: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- average_temperature: double (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp_country.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-------------------+-------+\n",
      "|cntyl_id|year|average_temperature|country|\n",
      "+--------+----+-------------------+-------+\n",
      "|     109|1819|              5.059|Estonia|\n",
      "|     109|1921|              5.187|Estonia|\n",
      "|     109|2011|              6.865|Estonia|\n",
      "|     109|1764|              5.116|Estonia|\n",
      "|     109|1984|              5.747|Estonia|\n",
      "|     109|1830|              4.195|Estonia|\n",
      "|     109|1750|              5.802|Estonia|\n",
      "|     116|1837|              8.951|Ireland|\n",
      "|     116|1820|              8.829|Ireland|\n",
      "|     116|1984|              9.785|Ireland|\n",
      "|     116|1976|              9.862|Ireland|\n",
      "|     116|1927|              9.415|Ireland|\n",
      "|     116|1935|               9.67|Ireland|\n",
      "|     116|1772|              9.038|Ireland|\n",
      "|     165|1893|             10.541|Croatia|\n",
      "|     165|1952|             11.888|Croatia|\n",
      "|     165|1899|             11.196|Croatia|\n",
      "|     165|1770|             11.085|Croatia|\n",
      "|     165|1786|             10.581|Croatia|\n",
      "|     165|1907|             11.052|Croatia|\n",
      "+--------+----+-------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp_country.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transport = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(OUTPUTFOLDER, 'transport.parquet')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transport.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(OUTPUTFOLDER, 'dates.parquet')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## city demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_demographics = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(OUTPUTFOLDER, 'city_demographics.parquet')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_demographics.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_demographics.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(OUTPUTFOLDER, 'dates.parquet')) \n",
    "dates.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visa = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(OUTPUTFOLDER, 'visa.parquet')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visa.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visa = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join('/Users/christian/Data/udacity_capstone/output2/output_test/visa.parquet')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visa.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## temperature_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_global = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(OUTPUTFOLDER, 'temperature_global.parquet')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_global.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_global.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_global.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fact Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "immi = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(OUTPUTFOLDER, 'immigration.parquet')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "immi.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "immi.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "immi.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
