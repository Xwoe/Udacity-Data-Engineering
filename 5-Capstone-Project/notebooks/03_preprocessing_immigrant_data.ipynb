{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from itertools import chain\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAFOLDER = '/Users/christian/Data/udacity_capstone/'\n",
    "OUTPUTFOLDER = '/Users/christian/Data/udacity_capstone/output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_immi_schema():\n",
    "    \"\"\"\n",
    "    Map the column names to datatypes and return as a schema.\n",
    "    \"\"\"\n",
    "    immi_schema = T.StructType([\n",
    "        T.StructField('cicid', T.IntegerType()),\n",
    "        T.StructField('i94yr', T.StringType()),\n",
    "        T.StructField('i94mon', T.StringType()),\n",
    "        T.StructField('i94cit', T.StringType()),\n",
    "        T.StructField('i94res', T.IntegerType()),\n",
    "        T.StructField('i94port', T.StringType()),\n",
    "        T.StructField('arrdate', T.StringType()),\n",
    "        T.StructField('i94mode', T.StringType()),\n",
    "        T.StructField('i94addr', T.StringType()),\n",
    "        T.StructField('depdate', T.StringType()),\n",
    "        T.StructField('i94bir', T.StringType()),\n",
    "        T.StructField('i94visa', T.StringType()),\n",
    "        T.StructField('count',  T.IntegerType()),\n",
    "        T.StructField('dtadfile', T.StringType()),\n",
    "        T.StructField('visapost', T.IntegerType()),\n",
    "        T.StructField('occup', T.StringType()),\n",
    "        T.StructField('entdepa', T.StringType()),\n",
    "        T.StructField('entdepd', T.IntegerType()),\n",
    "        T.StructField('entdepu', T.StringType()),\n",
    "        T.StructField('matflag', T.StringType()),\n",
    "        T.StructField('biryear', T.StringType()),\n",
    "        T.StructField('dtaddto', T.StringType()),\n",
    "        T.StructField('gender', T.StringType()),\n",
    "        T.StructField('insnum', T.StringType()),\n",
    "        T.StructField('airline', T.StringType()),\n",
    "        T.StructField('admnum', T.StringType()),\n",
    "        T.StructField('fltno', T.StringType()),\n",
    "        T.StructField('visatype', T.StringType()),\n",
    "    ])\n",
    "    return immi_schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = get_immi_schema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "#df_spark = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "df_spark = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(DATAFOLDER, 'sas_data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>depdate</th>\n",
       "      <td>103422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94mode</th>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94bir</th>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>biryear</th>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cicid</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94yr</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94mon</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94cit</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94res</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arrdate</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i94visa</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>admnum</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          count\n",
       "depdate  103422\n",
       "i94mode     102\n",
       "i94bir       19\n",
       "biryear      19\n",
       "cicid         0\n",
       "i94yr         0\n",
       "i94mon        0\n",
       "i94cit        0\n",
       "i94res        0\n",
       "arrdate       0\n",
       "i94visa       0\n",
       "count         0\n",
       "admnum        0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_missings(df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "|   cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|         admnum|fltno|visatype|\n",
      "+--------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "|459651.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|     FL|20559.0|  54.0|    2.0|  1.0|20160403|    null| null|      O|      R|   null|      M| 1962.0|07012016|  null|  null|     VS|5.5556253633E10|00115|      WT|\n",
      "|459652.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|     FL|20555.0|  74.0|    2.0|  1.0|20160403|    null| null|      T|      O|   null|      M| 1942.0|07012016|     F|  null|     VS|   6.74406485E8|  103|      WT|\n",
      "|459653.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|     FL|20557.0|  44.0|    2.0|  1.0|20160403|    null| null|      T|      Q|   null|      M| 1972.0|10022016|     M|  null|     VS|   6.74948185E8|  109|      B2|\n",
      "|459654.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|      G|20555.0|  38.0|    2.0|  1.0|20160403|    null| null|      O|      O|   null|      M| 1978.0|07012016|  null|  null|     VS|5.5541762033E10|00103|      WT|\n",
      "|459655.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|     GA|   null|  64.0|    2.0|  1.0|20160403|    null| null|      G|   null|   null|   null| 1952.0|07012016|     F|  null|     VS|5.5541328433E10|00103|      WT|\n",
      "|459656.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|     GA|   null|  63.0|    2.0|  1.0|20160403|    null| null|      G|   null|   null|   null| 1953.0|07012016|     M|  null|     BA|5.5578035433E10|00227|      WT|\n",
      "|459657.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|     GA|20548.0|  44.0|    1.0|  1.0|20160403|    null| null|      G|      I|   null|      M| 1972.0|07012016|     M|  null|     BA|5.5578919033E10|00227|      WB|\n",
      "|459658.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|     GA|20548.0|  39.0|    1.0|  1.0|20160403|    null| null|      G|      I|   null|      M| 1977.0|07012016|     M|  null|     BA|5.5578743033E10|00227|      WB|\n",
      "|459659.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|     GA|20573.0|  84.0|    2.0|  1.0|20160403|    null| null|      G|      I|   null|      M| 1932.0|07012016|     M|  null|     BA|5.5577459833E10|00227|      WT|\n",
      "|459660.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|     GA|20555.0|  55.0|    2.0|  1.0|20160403|    null| null|      G|      N|   null|      M| 1961.0|07012016|     M|  null|     BA|5.5577874033E10|00227|      WT|\n",
      "|459661.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|     GA|20555.0|  54.0|    2.0|  1.0|20160403|    null| null|      G|      N|   null|      M| 1962.0|07012016|     M|  null|     BA|5.5577725633E10|00227|      WT|\n",
      "|459662.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|     GA|20548.0|  36.0|    2.0|  1.0|20160403|    null| null|      G|      O|   null|      M| 1980.0|07012016|     M|  null|     DL|5.5577644033E10|00349|      WT|\n",
      "|459663.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|     GA|20549.0|  71.0|    1.0|  1.0|20160403|    null| null|      G|      O|   null|      M| 1945.0|07012016|     M|  null|     DL|5.5574698833E10|00031|      WB|\n",
      "|459664.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|     GA|20549.0|  39.0|    2.0|  1.0|20160403|    null| null|      G|      O|   null|      M| 1977.0|07012016|     M|  null|     VS|5.5560145833E10|00109|      WT|\n",
      "|459665.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|     GA|20550.0|  54.0|    1.0|  1.0|20160403|    null| null|      G|      O|   null|      M| 1962.0|07012016|     M|  null|     BA|5.5577579933E10|00227|      WB|\n",
      "|459666.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|     GA|20550.0|  51.0|    1.0|  1.0|20160403|    null| null|      G|      O|   null|      M| 1965.0|07012016|     M|  null|     BA|5.5578365433E10|00227|      WB|\n",
      "|459667.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|     GA|20550.0|  43.0|    1.0|  1.0|20160403|    null| null|      G|      O|   null|      M| 1973.0|07012016|     M|  null|     DL|5.5574951833E10|00031|      WB|\n",
      "|459668.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|     GA|20550.0|  37.0|    1.0|  1.0|20160403|    null| null|      G|      O|   null|      M| 1979.0|07012016|     F|  null|     DL|5.5564568733E10|00029|      WB|\n",
      "|459669.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|     GA|20550.0|  33.0|    1.0|  1.0|20160403|    null| null|      G|      O|   null|      M| 1983.0|07012016|     F|  null|     VS|5.5557739533E10|00109|      WB|\n",
      "|459670.0|2016.0|   4.0| 135.0| 135.0|    ATL|20547.0|    1.0|     GA|20550.0|  50.0|    1.0|  1.0|20160403|    null| null|      G|      O|   null|      M| 1966.0|07012016|     M|  null|     DL|5.5574648333E10|00031|      WB|\n",
      "+--------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check uniqueness of admission number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2641028"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2641028"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.select('cicid').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2641028"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.drop_duplicates().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = df_spark.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2641028"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.count() == df_spark.select('admnum').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2637526"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.select('admnum').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate_admnum = df_spark.groupBy('admnum').count().filter('count > 2').orderBy(F.desc('count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89977239030.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_admnum.first()['admnum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "| 155049.0|2016.0|   4.0| 582.0| 582.0|    BRO|20545.0|    1.0|     TX|20552.0|  32.0|    1.0|  1.0|20160401|     MER| null|      H|      R|   null|      M| 1984.0|09062016|     M|  null|    *GA|8.997723903E10|N934C|      B1|\n",
      "|3241942.0|2016.0|   4.0| 582.0| 582.0|    BRO|20561.0|    1.0|     TX|20567.0|  32.0|    1.0|  1.0|20160417|     MER| null|      H|      R|   null|      M| 1984.0|09062016|     M|  null|    *GA|8.997723903E10|N934C|      B1|\n",
      "|2433523.0|2016.0|   4.0| 582.0| 582.0|    BRO|20557.0|    1.0|     TX|20560.0|  32.0|    1.0|  1.0|20160413|     MER| null|      H|      R|   null|      M| 1984.0|09062016|     M|  null|    *GA|8.997723903E10|N934C|      B1|\n",
      "|3015990.0|2016.0|   4.0| 582.0| 582.0|    BRO|20560.0|    1.0|     TX|20561.0|  32.0|    1.0|  1.0|20160416|     MER| null|      H|      R|   null|      M| 1984.0|09062016|     M|  null|    *GA|8.997723903E10|N934C|      B1|\n",
      "|1918519.0|2016.0|   4.0| 582.0| 582.0|    SPE|20554.0|    1.0|     TX|20557.0|  32.0|    1.0|  1.0|20160410|     MER| null|      H|      R|   null|      M| 1984.0|09062016|     M|  null|    *GA|8.997723903E10|N934C|      B1|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark.filter('admnum == 89977239030').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = df_spark.withColumn('i94yr', df_spark['i94yr'].cast(T.IntegerType())).\\\n",
    "        withColumn('i94mon', df_spark['i94mon'].cast(T.IntegerType())).\\\n",
    "        withColumn('i94cit', df_spark['i94cit'].cast(T.IntegerType())).\\\n",
    "        withColumn('i94res', df_spark['i94res'].cast(T.IntegerType())).\\\n",
    "        withColumn('arrdate', df_spark['arrdate'].cast(T.IntegerType())).\\\n",
    "        withColumn('i94mode', df_spark['i94mode'].cast(T.IntegerType())).\\\n",
    "        withColumn('depdate', df_spark['depdate'].cast(T.IntegerType())).\\\n",
    "        withColumn('i94bir', df_spark['i94bir'].cast(T.IntegerType())).\\\n",
    "        withColumn('i94visa', df_spark['i94visa'].cast(T.IntegerType())).\\\n",
    "        withColumn('count', df_spark['count'].cast(T.IntegerType())).\\\n",
    "        withColumn('biryear', df_spark['biryear'].cast(T.IntegerType())).\\\n",
    "        withColumn('admnum', df_spark['admnum'].cast(T.IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = pd.read_csv(os.path.join(DATAFOLDER, 'immigration_data_sample.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time = df_spark\n",
    "df_time = df_time.withColumn(\"arrival_date\", F.expr(\"date_add(to_date('1960-01-01'), arrdate)\"))\n",
    "df_time = df_time.withColumn(\"depart_date\", F.expr(\"date_add(to_date('1960-01-01'), depdate)\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time.withColumn(\"diff_days\", F.datediff(\"depart_date\", \"arrival_date\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## map i94addrl - state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def map_col(df, map_col_name, df_col_name, new_col_name):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : spark dataframe\n",
    "        The file containing the df_col_name to be used for mapping.\n",
    "    map_col_name : str\n",
    "        The column name of the mapping file.\n",
    "    df_col_name : str\n",
    "        The column name in the Spark dataframe to be used.\n",
    "    new_col_name : str\n",
    "        New column name of the mapping results.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    df_map = pd.read_csv(os.path.join(DATAFOLDER, f'{map_col_name}.csv'), quotechar=\"'\")\n",
    "    id_col = f'{map_col_name}_id'\n",
    "    dic_map = dict(zip(df_map[id_col], df_map[map_col_name]))\n",
    "    mapping_expr = F.create_map([F.lit(x) for x in chain(*dic_map.items())])\n",
    "    return df.withColumn(new_col_name, mapping_expr[F.col(df_col_name)])\n",
    "\n",
    "\n",
    "\n",
    "@udf\n",
    "def udf_city_name(city_full):\n",
    "    splt = str.split(city_full, ',')\n",
    "    if len(splt) == 0:\n",
    "        return ''\n",
    "    return splt[0].capitalize().strip()\n",
    "\n",
    "@udf\n",
    "def udf_state_short(city_full):\n",
    "    splt = str.split(city_full, ',')\n",
    "    if len(splt) < 2:\n",
    "        return ''\n",
    "    return splt[1].strip()\n",
    "\n",
    "@udf\n",
    "def udf_state_format(port_state):\n",
    "    return port_state.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.select(\"i94addr\").distinct().show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO replace mapping in i94addrl to have names in captialize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df, map_col_name, df_col_name, new_col_name\n",
    "df_test = df_time\n",
    "#df_test = map_col(df_test, 'i94prtl', 'i94port', 'port')\n",
    "\n",
    "# city name of arrival port\n",
    "#df_test = df_test.withColumn(\"port_city\", udf_city_name(\"port\"))\n",
    "df_test = map_col(df_test, 'prtl_city', 'i94port', 'port_city')\n",
    "#df_test = df_test.withColumn(\"port_state_short\", udf_state_short(\"port\"))\n",
    "df_test = map_col(df_test, 'prtl_state', 'i94port', 'port_state_short')\n",
    "df_test = map_col(df_test, 'addrl', 'port_state_short', 'port_state')\n",
    "#df_test = df_test.withColumn('port_state_2', udf_state_format('port_state'))\n",
    "\n",
    "# country of entry, country of citizenship\n",
    "df_test = map_col(df_test, 'cntyl', 'i94port', 'state_cit')\n",
    "df_test = map_col(df_test, 'cntyl', 'i94port', 'state_res')\n",
    "#df_test = map_col(df_test, 'i94addrl', 'i94addr', 'port_state')\n",
    "df_test = map_col(df_test, 'visa', 'i94visa', 'visa')\n",
    "#df_test = df_test.withColumn(\"port_state\", udf_state_format(\"port_state\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is arrdate ever null?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.where(F.col(\"arrdate\").isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge with demographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#demographic = pd.read_csv(os.path.join(DATAFOLDER, 'us-cities-demographics.csv'), sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics = spark.read.format('csv').options(header='true', sep=';', inferSchema=True).\\\n",
    "    load(os.path.join(DATAFOLDER, 'us-cities-demographics.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "match airport with demographic table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up unique combinations of city and state in fact table\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset with unique combinations of city, state and \n",
    "# port_city, port_state, i94port (as key )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_read_csv(spark, folder, filename, **kwargs):\n",
    "\n",
    "    return spark.read.format('csv').options(header='true', inferSchema=True, **kwargs).\\\n",
    "        load(os.path.join(folder, filename))\n",
    "\n",
    "def format_column_names(s):\n",
    "    s = s.casefold()\n",
    "    s = s.replace(' ', '_')\n",
    "    s = s.replace('-', '_')\n",
    "    return s\n",
    "\n",
    "def rename_columns(df):\n",
    "    old_names = df.schema.names\n",
    "    new_names = [format_column_names(s) for s in old_names]\n",
    "    df = reduce(lambda df, idx: \n",
    "                df.withColumnRenamed(old_names[idx], new_names[idx]), range(len(old_names)), df)\n",
    "    return df\n",
    "\n",
    "def create_demographic_table(spark, datafolder, outputfolder): \n",
    "    # select a subset of original table\n",
    "    #df_demo = df.select(['i94port']).dropDuplicates()\n",
    "    df_demo = spark_read_csv(spark, datafolder, 'prtl_city.csv')\n",
    "    #pd.read_csv(os.path.join(datafolder, 'prtl.csv'), quotechar=\"'\")\n",
    "    demographics = spark_read_csv(spark, datafolder, 'us-cities-demographics.csv', sep=';')\n",
    "    #pd.read_csv(os.path.join(datafolder, 'us-cities-demographics.csv'), quotechar=\"'\")\n",
    "    \n",
    "    # do the preprocessing, append columns\n",
    "    #df_demo = map_col(df_demo, 'prtl_city', 'prtl_city', 'port_city')\n",
    "    df_demo = df_demo.withColumnRenamed('prtl_city', 'port_city')\n",
    "    #df_test = df_test.withColumn(\"port_state_short\", udf_state_short(\"port\"))\n",
    "    df_demo = map_col(df_demo, 'prtl_state', 'prtl_city_id', 'port_state_short')\n",
    "    df_demo = map_col(df_demo, 'addrl', 'port_state_short', 'port_state')\n",
    "    \n",
    "    # we are deleting the ones, for which no states were found, since we only focus on the US here\n",
    "    # later expand world wide\n",
    "    df_demo = df_demo.dropna(subset=['port_state'])\n",
    "    df_demo = df_demo.join(demographics, (df_demo.port_city == demographics.City) & (df_demo.port_state_short == demographics['State Code']))\n",
    "    columns_to_drop = ['port_city', 'port_state', 'port_state_short']\n",
    "    df_demo = df_demo.drop(*columns_to_drop)\n",
    "    \n",
    "    # reformat column names\n",
    "    df_demo = rename_columns(df_demo)\n",
    "    df_demo.write.parquet(os.path.join(outputfolder, 'city_demographics.parquet'), 'overwrite')\n",
    "    return df_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo = create_demographic_table(spark, DATAFOLDER, OUTPUTFOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create single tables for everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## country table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_read_csv(folder, filename, **kwargs):\n",
    "\n",
    "    return spark.read.format('csv').options(header='true', inferSchema=True, **kwargs).\\\n",
    "        load(os.path.join(folder, filename))\n",
    "\n",
    "def csv_to_parquet(datafolder, outputfolder, csv_name, table_name): \n",
    "    df = spark_read_csv(datafolder, f'{csv_name}.csv', sep=',', quotechar=\"'\")\n",
    "    df = df.withColumnRenamed('value', 'id')\n",
    "    df.write.parquet(os.path.join(outputfolder, f'{table_name}.parquet'), 'overwrite')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntyl = csv_to_parquet(DATAFOLDER, OUTPUTFOLDER, 'i94cntyl', 'country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_col_name = 'prtl_state'\n",
    "df_map = spark_read_csv(spark, datafolder, f'{map_col_name}.csv')\n",
    "#pd.read_csv(os.path.join(DATAFOLDER, f'{map_col_name}.csv'), quotechar=\"'\")\n",
    "df_map = df_map.toPandas()\n",
    "id_col = f'{map_col_name}_id'\n",
    "dic_map = dict(zip(df_map[id_col], df_map[map_col_name]))\n",
    "mapping_expr = F.create_map([F.lit(x) for x in chain(*dic_map.items())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_map[id_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model table i94model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transport = csv_to_parquet(DATAFOLDER, OUTPUTFOLDER, 'i94model', 'transport')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transport.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visa table i94visa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visa = csv_to_parquet(DATAFOLDER, OUTPUTFOLDER, 'i94visa', 'visa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visa.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## global annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "# also aggregate by year\n",
    "folder = os.path.join(DATAFOLDER, 'climate-change-earth-surface-temperature-data')\n",
    "global_annual = csv_to_parquet(folder, OUTPUTFOLDER, 'GlobalTemperatures', 'visa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_full_time_table(outputfolder, daysafter=36525):\n",
    "\n",
    "    days_till_2060 = range(daysafter)\n",
    "    all_dates = [(t,) for t in days_till_2060]\n",
    "    \n",
    "    t_schema = T.StructType([T.StructField('i94_date', T.IntegerType())])\n",
    "    timeframe = spark.createDataFrame(all_dates, t_schema)\n",
    "    timeframe = timeframe.withColumn(\"dt_date\", F.expr(\"date_add(to_date('1960-01-01'), i94_date)\"))\n",
    "    timeframe = timeframe.select('i94_date', 'dt_date',\n",
    "                    F.year('dt_date').alias('year'),\n",
    "                    F.month('dt_date').alias('month'),\n",
    "                    F.dayofmonth('dt_date').alias('day'),\n",
    "                    F.dayofweek('dt_date').alias('weekday'))\n",
    "    \n",
    "    timeframe.write.partitionBy('year', 'month').parquet(os.path.join(outputfolder, 'dates.parquet'), 'overwrite')\n",
    "    return timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeframe = create_full_time_table(OUTPUTFOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeframe.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeframe.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_full_time_table(outputfolder, daysafter=36525):\n",
    "\n",
    "    future_days = range(daysafter)\n",
    "    all_dates = [(t,) for t in future_days]\n",
    "    \n",
    "    t_schema = T.StructType([T.StructField('i_date', T.IntegerType())])\n",
    "    timeframe = spark.createDataFrame(all_dates, t_schema)\n",
    "    timeframe = timeframe.withColumn(\"dt_date\", F.expr(\"date_add(to_date('1960-01-01'), i_date)\"))\n",
    "    timeframe = timeframe.select('i_date', 'dt_date',\n",
    "                    F.year('dt_date').alias('year'),\n",
    "                    F.month('dt_date').alias('month'),\n",
    "                    F.dayofmonth('dt_date').alias('day'),\n",
    "                    F.dayofweek('dt_date').alias('weekday'))\n",
    "    \n",
    "    timeframe.write.partitionBy('year', 'month').parquet(os.path.join(outputfolder, 'dates.parquet'), 'overwrite')\n",
    "    return timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = create_full_time_table(OUTPUTFOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO apply rolling window function on average of last 10 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = os.path.join(DATAFOLDER, 'climate-change-earth-surface-temperature-data')\n",
    "filename = 'GlobalLandTemperaturesByCountry.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate = spark_read_csv(folder, filename, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert timestamp to date\n",
    "already done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate = climate.withColumn('dt', F.to_date(F.col('dt')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge with country id from country table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate.agg({\"dt\": \"min\"}).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate.agg({\"dt\": \"max\"}).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(OUTPUTFOLDER, 'country.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate = climate.join(country, on=country['i94cntyl'] == climate['Country'], how='leftouter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate = climate.drop('i94cntyl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_climate_country(folder, filename, outputfolder):\n",
    "    climate = spark_read_csv(folder, filename, sep=',')\n",
    "    climate = climate.withColumn('dt', F.to_date(F.col('dt')))\n",
    "    country = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(outputfolder, 'country.parquet'))\n",
    "    climate = climate.join(country, on=country['i94cntyl'] == climate['Country'], how='leftouter')\n",
    "    climate = climate.drop('i94cntyl')\n",
    "    climate = rename_columns(climate)\n",
    "    climate = climate.withColumn('year', F.year('dt').alias('year'))\n",
    "    climate = climate.withColumnRenamed('id', 'country_id').\\\n",
    "        withColumnRenamed('averagetemperatureuncertainty', 'avg_uncertainty').\\\n",
    "        withColumnRenamed('averagetemperature', 'avg_temperature')\n",
    "    climate.write.partitionBy('year', 'country').parquet(os.path.join(outputfolder, 'climate_country.parquet'), 'overwrite')\n",
    "    return climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = os.path.join(DATAFOLDER, 'climate-change-earth-surface-temperature-data')\n",
    "filename = 'GlobalLandTemperaturesByCountry.csv'\n",
    "climate_country = generate_climate_country(folder, filename, OUTPUTFOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_country = climate_country.withColumnRenamed('id', 'country_id').\\\n",
    "    withColumnRenamed('averagetemperatureuncertainty', 'avg_uncertainty')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aggregate annual average temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual = climate.withColumn('year', F.year('dt').alias('year'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_country.groupby([F.col('country'), F.col('country_id'), F.col('year')]).agg(F.avg('averagetemperature').alias('avg_temperature')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(OUTPUTFOLDER, 'climate_country.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_annual_temp_table(outputfolder):\n",
    "    annual = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(OUTPUTFOLDER, 'climate_country.parquet'))\n",
    "    annual.groupby([F.col('country'), F.col('country_id'), F.col('year')]).agg(F.avg('avg_temperature').alias('avg_temperature'))\n",
    "    annual.write.partitionBy('country').parquet(os.path.join(outputfolder, 'annual_climate_country.parquet'), 'overwrite')\n",
    "    return annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(OUTPUTFOLDER, 'climate_country.parquet'))\n",
    "annual.groupby([F.col('country'), F.col('country_id'), F.col('year')]).agg(F.avg('averagetemperature').alias('avg_temperature'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo when storing partition by country and year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asylum Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/dhs/refugee-report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fact Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate length of stay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeframe.filter(\"i94_date = 20573\").select('dt_date').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact = df_test.withColumn(\"arrival_dt\", F.expr(\"date_add(to_date('1960-01-01'), arrdate)\"))\n",
    "df_fact = df_fact.withColumn(\"depart_dt\", F.expr(\"date_add(to_date('1960-01-01'), depdate)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_columns = ['cicid', 'i94yr', 'i94mon', 'arrival_dt', 'arrdate', 'depdate', 'i94cit', 'i94res', 'i94port', 'i94mode', 'i94addr', 'i94bir',\n",
    "               'i94visa', 'visatype', 'biryear', 'gender', 'airline', 'fltno', 'length_stay']\n",
    "\n",
    "#maybe = ['entdepa', 'entdepd', 'matflag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact = df_fact.withColumn(\"length_stay\", F.datediff(\"depart_date\", \"arrival_date\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact.select(keep_columns).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_facts_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test.join(timeframe, on=df_test['arrdate']==timeframe['i94_date'], how='leftouter').show() \n",
    "# too complicated\n",
    "#df_test.join(timeframe, on=df_test['arrdate']==timeframe['i94_date'], how='left').dropDuplicates().select(timeframe['dt_date']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.withColumn(\"length_stay\", F.datediff(\"depart_date\", \"arrival_date\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# TODO\n",
    "\n",
    "- copy my csv files to S3 manually\n",
    "with a DAG\n",
    "- write some csvs to redshift \n",
    "- copy the stuff to redshift\n",
    "- preprocessed tables as parquet files?\n",
    "- maybe split it up into two DAGs\n",
    "-- one does the intialization of the dimension tables, which won't change that often\n",
    "-- the other one does the updating of the fact table\n",
    "\n",
    "- add IATA code to city ?\n",
    "\n",
    "- or maybe the nodes can be confiured to only run once?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. csv von S3 -> redshift\n",
    "2. parquet von S3 -> Spark -> redshift, maybe like this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://sonra.io/2018/01/01/using-apache-airflow-to-build-a-data-pipeline-on-aws/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COPY listing\n",
    "FROM 's3://mybucket/data/listings/parquet/'\n",
    "IAM_ROLE 'arn:aws:iam::0123456789012:role/MyRedshiftRole'\n",
    "FORMAT AS PARQUET;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If nothing works use Docker\n",
    "https://towardsdatascience.com/getting-started-with-airflow-using-docker-cd8b44dbff98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional databases\n",
    "- https://www.kaggle.com/open-flights/flight-route-database\n",
    "- https://www.kaggle.com/dhs/refugee-report\n",
    "- https://www.dhs.gov/immigration-statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUTFOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "def count_missings(spark_df,sort=True):\n",
    "    \"\"\"\n",
    "    Counts number of nulls and nans in each column\n",
    "    \"\"\"\n",
    "    df = spark_df.select([F.count(F.when(F.isnan(c) | F.isnull(c), c)).alias(c) for (c,c_type) in spark_df.dtypes if c_type not in ('timestamp', 'string', 'date')]).toPandas()\n",
    "\n",
    "    if len(df) == 0:\n",
    "        print(\"There are no any missing values!\")\n",
    "        return None\n",
    "\n",
    "    if sort:\n",
    "        return df.rename(index={0: 'count'}).T.sort_values(\"count\",ascending=False)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviewing all the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(OUTPUTFOLDER, 'country.parquet')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cntyl_id', 'cntyl']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_country.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|cntyl_id|               cntyl|\n",
      "+--------+--------------------+\n",
      "|     582|Mexico Air Sea, A...|\n",
      "|     236|         Afghanistan|\n",
      "|     101|             Albania|\n",
      "|     316|             Algeria|\n",
      "|     102|             Andorra|\n",
      "|     324|              Angola|\n",
      "|     529|            Anguilla|\n",
      "|     518|     Antigua-barbuda|\n",
      "|     687|          Argentina |\n",
      "|     151|             Armenia|\n",
      "|     532|               Aruba|\n",
      "|     438|           Australia|\n",
      "|     103|             Austria|\n",
      "|     152|          Azerbaijan|\n",
      "|     512|             Bahamas|\n",
      "|     298|             Bahrain|\n",
      "|     274|          Bangladesh|\n",
      "|     513|            Barbados|\n",
      "|     104|             Belgium|\n",
      "|     581|              Belize|\n",
      "+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_country.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## temperature_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_country = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(OUTPUTFOLDER, 'temperature_country.parquet')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dt',\n",
       " 'average_temperature',\n",
       " 'average_temperature_uncertainty',\n",
       " 'cntyl_id',\n",
       " 'year',\n",
       " 'country']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp_country.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: date (nullable = true)\n",
      " |-- average_temperature: double (nullable = true)\n",
      " |-- average_temperature_uncertainty: double (nullable = true)\n",
      " |-- cntyl_id: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp_country.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------------------+--------+----+---------+\n",
      "|        dt|average_temperature|average_temperature_uncertainty|cntyl_id|year|  country|\n",
      "+----------+-------------------+-------------------------------+--------+----+---------+\n",
      "|1967-01-01|             25.488|                          0.227|     386|1967|    Benin|\n",
      "|1967-02-01|             28.826|                          0.309|     386|1967|    Benin|\n",
      "|1967-03-01|             29.586|                          0.304|     386|1967|    Benin|\n",
      "|1967-04-01|             29.474|                          0.271|     386|1967|    Benin|\n",
      "|1967-05-01|             28.676|                          0.349|     386|1967|    Benin|\n",
      "|1967-06-01|               26.5|                          0.309|     386|1967|    Benin|\n",
      "|1967-07-01|             25.449|                          0.395|     386|1967|    Benin|\n",
      "|1967-08-01|              24.73|                          0.299|     386|1967|    Benin|\n",
      "|1967-09-01|             25.239|                          0.271|     386|1967|    Benin|\n",
      "|1967-10-01|             26.405|                          0.084|     386|1967|    Benin|\n",
      "|1967-11-01|             26.791|                          0.173|     386|1967|    Benin|\n",
      "|1967-12-01|             26.549|                          0.212|     386|1967|    Benin|\n",
      "|1932-01-01|             22.518|                          0.522|     351|1932|Swaziland|\n",
      "|1932-02-01|             22.651|                          0.399|     351|1932|Swaziland|\n",
      "|1932-03-01|             21.341|                          0.392|     351|1932|Swaziland|\n",
      "|1932-04-01|             19.832|                          0.469|     351|1932|Swaziland|\n",
      "|1932-05-01|             15.701|                          0.649|     351|1932|Swaziland|\n",
      "|1932-06-01|             13.798|                          0.388|     351|1932|Swaziland|\n",
      "|1932-07-01|              13.06|                          0.392|     351|1932|Swaziland|\n",
      "|1932-08-01|             15.901|                          0.416|     351|1932|Swaziland|\n",
      "+----------+-------------------+-------------------------------+--------+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp_country.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## temperature_annual_country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_country = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(OUTPUTFOLDER, 'temperature_annual_country.parquet')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cntyl_id', 'year', 'average_temperature', 'country']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp_country.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cntyl_id: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- average_temperature: double (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp_country.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+-------------------+-------+\n",
      "|cntyl_id|year|average_temperature|country|\n",
      "+--------+----+-------------------+-------+\n",
      "|     109|1819|              5.059|Estonia|\n",
      "|     109|1921|              5.187|Estonia|\n",
      "|     109|2011|              6.865|Estonia|\n",
      "|     109|1764|              5.116|Estonia|\n",
      "|     109|1984|              5.747|Estonia|\n",
      "|     109|1830|              4.195|Estonia|\n",
      "|     109|1750|              5.802|Estonia|\n",
      "|     116|1837|              8.951|Ireland|\n",
      "|     116|1820|              8.829|Ireland|\n",
      "|     116|1984|              9.785|Ireland|\n",
      "|     116|1976|              9.862|Ireland|\n",
      "|     116|1927|              9.415|Ireland|\n",
      "|     116|1935|               9.67|Ireland|\n",
      "|     116|1772|              9.038|Ireland|\n",
      "|     165|1893|             10.541|Croatia|\n",
      "|     165|1952|             11.888|Croatia|\n",
      "|     165|1899|             11.196|Croatia|\n",
      "|     165|1770|             11.085|Croatia|\n",
      "|     165|1786|             10.581|Croatia|\n",
      "|     165|1907|             11.052|Croatia|\n",
      "+--------+----+-------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temp_country.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transport = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(OUTPUTFOLDER, 'transport.parquet')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model_id', 'model']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_transport.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|model_id|       model|\n",
      "+--------+------------+\n",
      "|       1|         Air|\n",
      "|       2|         Sea|\n",
      "|       3|        Land|\n",
      "|       9|Not reported|\n",
      "+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_transport.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(OUTPUTFOLDER, 'dates.parquet')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i_date', 'dt_date', 'day', 'weekday', 'year', 'month']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_time.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---+-------+----+-----+\n",
      "|i_date|   dt_date|day|weekday|year|month|\n",
      "+------+----------+---+-------+----+-----+\n",
      "|  9709|1986-08-01|  1|      6|1986|    8|\n",
      "|  9710|1986-08-02|  2|      7|1986|    8|\n",
      "|  9711|1986-08-03|  3|      1|1986|    8|\n",
      "|  9712|1986-08-04|  4|      2|1986|    8|\n",
      "|  9713|1986-08-05|  5|      3|1986|    8|\n",
      "|  9714|1986-08-06|  6|      4|1986|    8|\n",
      "|  9715|1986-08-07|  7|      5|1986|    8|\n",
      "|  9716|1986-08-08|  8|      6|1986|    8|\n",
      "|  9717|1986-08-09|  9|      7|1986|    8|\n",
      "|  9718|1986-08-10| 10|      1|1986|    8|\n",
      "|  9719|1986-08-11| 11|      2|1986|    8|\n",
      "|  9720|1986-08-12| 12|      3|1986|    8|\n",
      "|  9721|1986-08-13| 13|      4|1986|    8|\n",
      "|  9722|1986-08-14| 14|      5|1986|    8|\n",
      "|  9723|1986-08-15| 15|      6|1986|    8|\n",
      "|  9724|1986-08-16| 16|      7|1986|    8|\n",
      "|  9725|1986-08-17| 17|      1|1986|    8|\n",
      "|  9726|1986-08-18| 18|      2|1986|    8|\n",
      "|  9727|1986-08-19| 19|      3|1986|    8|\n",
      "|  9728|1986-08-20| 20|      4|1986|    8|\n",
      "+------+----------+---+-------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_time.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## city demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_demographics = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(OUTPUTFOLDER, 'city_demographics.parquet')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prtl_city_id',\n",
       " 'city',\n",
       " 'median_age',\n",
       " 'male_population',\n",
       " 'female_population',\n",
       " 'total_population',\n",
       " 'foreignborn',\n",
       " 'average_household_size',\n",
       " 'state_code',\n",
       " 'state']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_demographics.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- prtl_city_id: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- median_age: double (nullable = true)\n",
      " |-- male_population: integer (nullable = true)\n",
      " |-- female_population: integer (nullable = true)\n",
      " |-- total_population: integer (nullable = true)\n",
      " |-- foreignborn: integer (nullable = true)\n",
      " |-- average_household_size: double (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "city_demographics.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+----------+---------------+-----------------+----------------+-----------+----------------------+----------+--------------+\n",
      "|prtl_city_id|            city|median_age|male_population|female_population|total_population|foreignborn|average_household_size|state_code|         state|\n",
      "+------------+----------------+----------+---------------+-----------------+----------------+-----------+----------------------+----------+--------------+\n",
      "|         COS|Colorado Springs|      34.8|         225544|           231018|          456562|      35320|                  2.48|        CO|      Colorado|\n",
      "|         WPB| West Palm Beach|      39.6|          49262|            57520|          106782|      30675|    2.5300000000000002|        FL|       Florida|\n",
      "|         FTL| Fort Lauderdale|      42.8|          93948|            84639|          178587|      47582|                  2.38|        FL|       Florida|\n",
      "|         SLC|  Salt Lake City|      32.1|          98364|            94296|          192660|      32166|                  2.38|        UT|          Utah|\n",
      "|         CRP|  Corpus Christi|      35.0|         160488|           163594|          324082|      30834|                  2.69|        TX|         Texas|\n",
      "|         SFR|   San Francisco|      38.3|         439752|           425064|          864816|     297199|                  2.37|        CA|    California|\n",
      "|         OKC|   Oklahoma City|      34.1|         309227|           322036|          631263|      80165|                  2.58|        OK|      Oklahoma|\n",
      "|         GRP|    Grand Rapids|      32.1|          95669|            99430|          195099|      19176|                  2.56|        MI|      Michigan|\n",
      "|         FAY|    Fayetteville|      30.7|         101051|           100914|          201965|      12863|                   2.5|        NC|North Carolina|\n",
      "|         JAC|    Jacksonville|      35.7|         419203|           448828|          868031|      85650|                  2.62|        FL|       Florida|\n",
      "|         PHI|    Philadelphia|      34.1|         741270|           826172|         1567442|     205339|                  2.61|        PA|  Pennsylvania|\n",
      "|         INP|    Indianapolis|      34.1|         410615|           437808|          848423|      72456|    2.5300000000000002|        IN|       Indiana|\n",
      "|         LKC|    Lake Charles|      31.8|          35678|            40388|           76066|       3215|                  2.36|        LA|     Louisiana|\n",
      "|         JER|     Jersey City|      34.3|         131765|           132512|          264277|     109186|    2.5700000000000003|        NJ|    New Jersey|\n",
      "|         LOS|     Los Angeles|      35.0|        1958998|          2012898|         3971896|    1485425|                  2.86|        CA|    California|\n",
      "|         KAN|     Kansas City|      35.9|         228430|           246931|          475361|      37787|                  2.35|        MO|      Missouri|\n",
      "|         ABQ|     Albuquerque|      36.0|         273323|           285808|          559131|      58200|                  2.49|        NM|    New Mexico|\n",
      "|         NOL|     New Orleans|      35.9|         185736|           203881|          389617|      21679|                  2.41|        LA|     Louisiana|\n",
      "|         BTN|     Baton Rouge|      31.2|         111492|           117104|          228596|      12268|                  2.52|        LA|     Louisiana|\n",
      "|         BRO|     Brownsville|      30.6|          87689|            96199|          183888|      53301|                  3.48|        TX|         Texas|\n",
      "+------------+----------------+----------+---------------+-----------------+----------------+-----------+----------------------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "city_demographics.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+---+-------+----+-----+\n",
      "|i_date|   dt_date|day|weekday|year|month|\n",
      "+------+----------+---+-------+----+-----+\n",
      "|  9709|1986-08-01|  1|      6|1986|    8|\n",
      "|  9710|1986-08-02|  2|      7|1986|    8|\n",
      "|  9711|1986-08-03|  3|      1|1986|    8|\n",
      "|  9712|1986-08-04|  4|      2|1986|    8|\n",
      "|  9713|1986-08-05|  5|      3|1986|    8|\n",
      "|  9714|1986-08-06|  6|      4|1986|    8|\n",
      "|  9715|1986-08-07|  7|      5|1986|    8|\n",
      "|  9716|1986-08-08|  8|      6|1986|    8|\n",
      "|  9717|1986-08-09|  9|      7|1986|    8|\n",
      "|  9718|1986-08-10| 10|      1|1986|    8|\n",
      "|  9719|1986-08-11| 11|      2|1986|    8|\n",
      "|  9720|1986-08-12| 12|      3|1986|    8|\n",
      "|  9721|1986-08-13| 13|      4|1986|    8|\n",
      "|  9722|1986-08-14| 14|      5|1986|    8|\n",
      "|  9723|1986-08-15| 15|      6|1986|    8|\n",
      "|  9724|1986-08-16| 16|      7|1986|    8|\n",
      "|  9725|1986-08-17| 17|      1|1986|    8|\n",
      "|  9726|1986-08-18| 18|      2|1986|    8|\n",
      "|  9727|1986-08-19| 19|      3|1986|    8|\n",
      "|  9728|1986-08-20| 20|      4|1986|    8|\n",
      "+------+----------+---+-------+----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dates = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(OUTPUTFOLDER, 'dates.parquet')) \n",
    "dates.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i_date', 'dt_date', 'day', 'weekday', 'year', 'month']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dates.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "visa = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(OUTPUTFOLDER, 'visa.parquet')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|visa_id|    visa|\n",
      "+-------+--------+\n",
      "|      1|Business|\n",
      "|      2|Pleasure|\n",
      "|      3| Student|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visa.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "visa = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join('/Users/christian/Data/udacity_capstone/output2/output_test/visa.parquet')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|visa_id|    visa|\n",
      "+-------+--------+\n",
      "|      1|Business|\n",
      "|      2|Pleasure|\n",
      "|      3| Student|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visa.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## temperature_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_global = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(OUTPUTFOLDER, 'temperature_global.parquet')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['year',\n",
       " 'land_average_temperature',\n",
       " 'land_average_temperature_uncertainty',\n",
       " 'land_max_temperature',\n",
       " 'land_max_temperature_uncertainty',\n",
       " 'land_min_temperature',\n",
       " 'land_min_temperature_uncertainty',\n",
       " 'land_and_ocean_average_temperature',\n",
       " 'land_and_ocean_average_temperature_uncertainty']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature_global.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- land_average_temperature: double (nullable = true)\n",
      " |-- land_average_temperature_uncertainty: double (nullable = true)\n",
      " |-- land_max_temperature: double (nullable = true)\n",
      " |-- land_max_temperature_uncertainty: double (nullable = true)\n",
      " |-- land_min_temperature: double (nullable = true)\n",
      " |-- land_min_temperature_uncertainty: double (nullable = true)\n",
      " |-- land_and_ocean_average_temperature: double (nullable = true)\n",
      " |-- land_and_ocean_average_temperature_uncertainty: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperature_global.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------------+------------------------------------+--------------------+--------------------------------+--------------------+--------------------------------+----------------------------------+----------------------------------------------+\n",
      "|year|land_average_temperature|land_average_temperature_uncertainty|land_max_temperature|land_max_temperature_uncertainty|land_min_temperature|land_min_temperature_uncertainty|land_and_ocean_average_temperature|land_and_ocean_average_temperature_uncertainty|\n",
      "+----+------------------------+------------------------------------+--------------------+--------------------------------+--------------------+--------------------------------+----------------------------------+----------------------------------------------+\n",
      "|1990|                    0.13|                               3.659|               0.086|                           0.057|              15.629|                          14.958|                             9.234|                                          0.12|\n",
      "+----+------------------------+------------------------------------+--------------------+--------------------------------+--------------------+--------------------------------+----------------------------------+----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temperature_global.where('year = 1990').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fact Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "immi = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(OUTPUTFOLDER, 'immigration.parquet')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cicid',\n",
       " 'i_yr',\n",
       " 'i_mon',\n",
       " 'arrdate',\n",
       " 'depdate',\n",
       " 'i_cit',\n",
       " 'i_res',\n",
       " 'i_port',\n",
       " 'i_mode',\n",
       " 'i_addr',\n",
       " 'i_bir',\n",
       " 'i_visa',\n",
       " 'visatype',\n",
       " 'gender',\n",
       " 'airline',\n",
       " 'fltno',\n",
       " 'length_stay']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immi.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----+-------+-------+-----+-----+------+------+------+-----+------+--------+------+-------+-----+-----------+\n",
      "| cicid|i_yr|i_mon|arrdate|depdate|i_cit|i_res|i_port|i_mode|i_addr|i_bir|i_visa|visatype|gender|airline|fltno|length_stay|\n",
      "+------+----+-----+-------+-------+-----+-----+------+------+------+-----+------+--------+------+-------+-----+-----------+\n",
      "|459651|2016|    4|  20547|  20559|  135|  135|   ATL|     1|    FL|   54|     2|      WT|  null|     VS|00115|         12|\n",
      "|459652|2016|    4|  20547|  20555|  135|  135|   ATL|     1|    FL|   74|     2|      WT|     F|     VS|  103|          8|\n",
      "|459653|2016|    4|  20547|  20557|  135|  135|   ATL|     1|    FL|   44|     2|      B2|     M|     VS|  109|         10|\n",
      "|459654|2016|    4|  20547|  20555|  135|  135|   ATL|     1|     G|   38|     2|      WT|  null|     VS|00103|          8|\n",
      "|459655|2016|    4|  20547|   null|  135|  135|   ATL|     1|    GA|   64|     2|      WT|     F|     VS|00103|       null|\n",
      "|459656|2016|    4|  20547|   null|  135|  135|   ATL|     1|    GA|   63|     2|      WT|     M|     BA|00227|       null|\n",
      "|459657|2016|    4|  20547|  20548|  135|  135|   ATL|     1|    GA|   44|     1|      WB|     M|     BA|00227|          1|\n",
      "|459658|2016|    4|  20547|  20548|  135|  135|   ATL|     1|    GA|   39|     1|      WB|     M|     BA|00227|          1|\n",
      "|459659|2016|    4|  20547|  20573|  135|  135|   ATL|     1|    GA|   84|     2|      WT|     M|     BA|00227|         26|\n",
      "|459660|2016|    4|  20547|  20555|  135|  135|   ATL|     1|    GA|   55|     2|      WT|     M|     BA|00227|          8|\n",
      "|459661|2016|    4|  20547|  20555|  135|  135|   ATL|     1|    GA|   54|     2|      WT|     M|     BA|00227|          8|\n",
      "|459662|2016|    4|  20547|  20548|  135|  135|   ATL|     1|    GA|   36|     2|      WT|     M|     DL|00349|          1|\n",
      "|459663|2016|    4|  20547|  20549|  135|  135|   ATL|     1|    GA|   71|     1|      WB|     M|     DL|00031|          2|\n",
      "|459664|2016|    4|  20547|  20549|  135|  135|   ATL|     1|    GA|   39|     2|      WT|     M|     VS|00109|          2|\n",
      "|459665|2016|    4|  20547|  20550|  135|  135|   ATL|     1|    GA|   54|     1|      WB|     M|     BA|00227|          3|\n",
      "|459666|2016|    4|  20547|  20550|  135|  135|   ATL|     1|    GA|   51|     1|      WB|     M|     BA|00227|          3|\n",
      "|459667|2016|    4|  20547|  20550|  135|  135|   ATL|     1|    GA|   43|     1|      WB|     M|     DL|00031|          3|\n",
      "|459668|2016|    4|  20547|  20550|  135|  135|   ATL|     1|    GA|   37|     1|      WB|     F|     DL|00029|          3|\n",
      "|459669|2016|    4|  20547|  20550|  135|  135|   ATL|     1|    GA|   33|     1|      WB|     F|     VS|00109|          3|\n",
      "|459670|2016|    4|  20547|  20550|  135|  135|   ATL|     1|    GA|   50|     1|      WB|     M|     DL|00031|          3|\n",
      "+------+----+-----+-------+-------+-----+-----+------+------+------+-----+------+--------+------+-------+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immi.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: integer (nullable = true)\n",
      " |-- i_yr: integer (nullable = true)\n",
      " |-- i_mon: integer (nullable = true)\n",
      " |-- arrdate: integer (nullable = true)\n",
      " |-- depdate: integer (nullable = true)\n",
      " |-- i_cit: integer (nullable = true)\n",
      " |-- i_res: integer (nullable = true)\n",
      " |-- i_port: string (nullable = true)\n",
      " |-- i_mode: integer (nullable = true)\n",
      " |-- i_addr: string (nullable = true)\n",
      " |-- i_bir: integer (nullable = true)\n",
      " |-- i_visa: integer (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- length_stay: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immi.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>depdate</th>\n",
       "      <td>103422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>length_stay</th>\n",
       "      <td>103422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i_mode</th>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i_bir</th>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cicid</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i_yr</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i_mon</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arrdate</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i_cit</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i_res</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i_visa</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              count\n",
       "depdate      103422\n",
       "length_stay  103422\n",
       "i_mode          102\n",
       "i_bir            19\n",
       "cicid             0\n",
       "i_yr              0\n",
       "i_mon             0\n",
       "arrdate           0\n",
       "i_cit             0\n",
       "i_res             0\n",
       "i_visa            0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_missings(immi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.039159751430124935"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "103422 / immi.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
