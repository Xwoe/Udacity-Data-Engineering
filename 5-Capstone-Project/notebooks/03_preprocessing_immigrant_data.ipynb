{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from itertools import chain\n",
    "from pyspark.sql.functions import udf\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAFOLDER = '/Users/christian/Data/udacity_capstone/'\n",
    "OUTPUTFOLDER = '/Users/christian/Data/udacity_capstone/output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_immi_schema():\n",
    "    \"\"\"\n",
    "    Map the column names to datatypes and return as a schema.\n",
    "    \"\"\"\n",
    "    immi_schema = T.StructType([\n",
    "        T.StructField('cicid', T.IntegerType()),\n",
    "        T.StructField('i94yr', T.StringType()),\n",
    "        T.StructField('i94mon', T.StringType()),\n",
    "        T.StructField('i94cit', T.StringType()),\n",
    "        T.StructField('i94res', T.IntegerType()),\n",
    "        T.StructField('i94port', T.StringType()),\n",
    "        T.StructField('arrdate', T.StringType()),\n",
    "        T.StructField('i94mode', T.StringType()),\n",
    "        T.StructField('i94addr', T.StringType()),\n",
    "        T.StructField('depdate', T.StringType()),\n",
    "        T.StructField('i94bir', T.StringType()),\n",
    "        T.StructField('i94visa', T.StringType()),\n",
    "        T.StructField('count',  T.IntegerType()),\n",
    "        T.StructField('dtadfile', T.StringType()),\n",
    "        T.StructField('visapost', T.IntegerType()),\n",
    "        T.StructField('occup', T.StringType()),\n",
    "        T.StructField('entdepa', T.StringType()),\n",
    "        T.StructField('entdepd', T.IntegerType()),\n",
    "        T.StructField('entdepu', T.StringType()),\n",
    "        T.StructField('matflag', T.StringType()),\n",
    "        T.StructField('biryear', T.StringType()),\n",
    "        T.StructField('dtaddto', T.StringType()),\n",
    "        T.StructField('gender', T.StringType()),\n",
    "        T.StructField('insnum', T.StringType()),\n",
    "        T.StructField('airline', T.StringType()),\n",
    "        T.StructField('admnum', T.StringType()),\n",
    "        T.StructField('fltno', T.StringType()),\n",
    "        T.StructField('visatype', T.StringType()),\n",
    "    ])\n",
    "    return immi_schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = get_immi_schema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "#df_spark = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "df_spark = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(DATAFOLDER, 'sas_data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark = df_spark.withColumn('i94yr', df_spark['i94yr'].cast(T.IntegerType())).\\\n",
    "        withColumn('i94mon', df_spark['i94mon'].cast(T.IntegerType())).\\\n",
    "        withColumn('i94cit', df_spark['i94cit'].cast(T.IntegerType())).\\\n",
    "        withColumn('i94res', df_spark['i94res'].cast(T.IntegerType())).\\\n",
    "        withColumn('arrdate', df_spark['arrdate'].cast(T.IntegerType())).\\\n",
    "        withColumn('i94mode', df_spark['i94mode'].cast(T.IntegerType())).\\\n",
    "        withColumn('depdate', df_spark['depdate'].cast(T.IntegerType())).\\\n",
    "        withColumn('i94bir', df_spark['i94bir'].cast(T.IntegerType())).\\\n",
    "        withColumn('i94visa', df_spark['i94visa'].cast(T.IntegerType())).\\\n",
    "        withColumn('count', df_spark['count'].cast(T.IntegerType())).\\\n",
    "        withColumn('biryear', df_spark['biryear'].cast(T.IntegerType())).\\\n",
    "        withColumn('admnum', df_spark['admnum'].cast(T.IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = pd.read_csv(os.path.join(DATAFOLDER, 'immigration_data_sample.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(cicid=459651.0, i94yr=2016, i94mon=4, i94cit=135, i94res=135, i94port='ATL', arrdate=20547, i94mode=1, i94addr='FL', depdate=20559, i94bir=54, i94visa=2, count=1, dtadfile='20160403', visapost=None, occup=None, entdepa='O', entdepd='R', entdepu=None, matflag='M', biryear=1962, dtaddto='07012016', gender=None, insnum=None, airline='VS', admnum=2147483647, fltno='00115', visatype='WT')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_spark.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time = df_spark\n",
    "df_time = df_time.withColumn(\"arrival_date\", F.expr(\"date_add(to_date('1960-01-01'), arrdate)\"))\n",
    "df_time = df_time.withColumn(\"depart_date\", F.expr(\"date_add(to_date('1960-01-01'), depdate)\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+----------+-----+--------+------------+-----------+---------+\n",
      "|   cicid|i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|    admnum|fltno|visatype|arrival_date|depart_date|diff_days|\n",
      "+--------+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+----------+-----+--------+------------+-----------+---------+\n",
      "|459651.0| 2016|     4|   135|   135|    ATL|  20547|      1|     FL|  20559|    54|      2|    1|20160403|    null| null|      O|      R|   null|      M|   1962|07012016|  null|  null|     VS|2147483647|00115|      WT|  2016-04-03| 2016-04-15|       12|\n",
      "|459652.0| 2016|     4|   135|   135|    ATL|  20547|      1|     FL|  20555|    74|      2|    1|20160403|    null| null|      T|      O|   null|      M|   1942|07012016|     F|  null|     VS| 674406485|  103|      WT|  2016-04-03| 2016-04-11|        8|\n",
      "|459653.0| 2016|     4|   135|   135|    ATL|  20547|      1|     FL|  20557|    44|      2|    1|20160403|    null| null|      T|      Q|   null|      M|   1972|10022016|     M|  null|     VS| 674948185|  109|      B2|  2016-04-03| 2016-04-13|       10|\n",
      "|459654.0| 2016|     4|   135|   135|    ATL|  20547|      1|      G|  20555|    38|      2|    1|20160403|    null| null|      O|      O|   null|      M|   1978|07012016|  null|  null|     VS|2147483647|00103|      WT|  2016-04-03| 2016-04-11|        8|\n",
      "|459655.0| 2016|     4|   135|   135|    ATL|  20547|      1|     GA|   null|    64|      2|    1|20160403|    null| null|      G|   null|   null|   null|   1952|07012016|     F|  null|     VS|2147483647|00103|      WT|  2016-04-03|       null|     null|\n",
      "|459656.0| 2016|     4|   135|   135|    ATL|  20547|      1|     GA|   null|    63|      2|    1|20160403|    null| null|      G|   null|   null|   null|   1953|07012016|     M|  null|     BA|2147483647|00227|      WT|  2016-04-03|       null|     null|\n",
      "|459657.0| 2016|     4|   135|   135|    ATL|  20547|      1|     GA|  20548|    44|      1|    1|20160403|    null| null|      G|      I|   null|      M|   1972|07012016|     M|  null|     BA|2147483647|00227|      WB|  2016-04-03| 2016-04-04|        1|\n",
      "|459658.0| 2016|     4|   135|   135|    ATL|  20547|      1|     GA|  20548|    39|      1|    1|20160403|    null| null|      G|      I|   null|      M|   1977|07012016|     M|  null|     BA|2147483647|00227|      WB|  2016-04-03| 2016-04-04|        1|\n",
      "|459659.0| 2016|     4|   135|   135|    ATL|  20547|      1|     GA|  20573|    84|      2|    1|20160403|    null| null|      G|      I|   null|      M|   1932|07012016|     M|  null|     BA|2147483647|00227|      WT|  2016-04-03| 2016-04-29|       26|\n",
      "|459660.0| 2016|     4|   135|   135|    ATL|  20547|      1|     GA|  20555|    55|      2|    1|20160403|    null| null|      G|      N|   null|      M|   1961|07012016|     M|  null|     BA|2147483647|00227|      WT|  2016-04-03| 2016-04-11|        8|\n",
      "|459661.0| 2016|     4|   135|   135|    ATL|  20547|      1|     GA|  20555|    54|      2|    1|20160403|    null| null|      G|      N|   null|      M|   1962|07012016|     M|  null|     BA|2147483647|00227|      WT|  2016-04-03| 2016-04-11|        8|\n",
      "|459662.0| 2016|     4|   135|   135|    ATL|  20547|      1|     GA|  20548|    36|      2|    1|20160403|    null| null|      G|      O|   null|      M|   1980|07012016|     M|  null|     DL|2147483647|00349|      WT|  2016-04-03| 2016-04-04|        1|\n",
      "|459663.0| 2016|     4|   135|   135|    ATL|  20547|      1|     GA|  20549|    71|      1|    1|20160403|    null| null|      G|      O|   null|      M|   1945|07012016|     M|  null|     DL|2147483647|00031|      WB|  2016-04-03| 2016-04-05|        2|\n",
      "|459664.0| 2016|     4|   135|   135|    ATL|  20547|      1|     GA|  20549|    39|      2|    1|20160403|    null| null|      G|      O|   null|      M|   1977|07012016|     M|  null|     VS|2147483647|00109|      WT|  2016-04-03| 2016-04-05|        2|\n",
      "|459665.0| 2016|     4|   135|   135|    ATL|  20547|      1|     GA|  20550|    54|      1|    1|20160403|    null| null|      G|      O|   null|      M|   1962|07012016|     M|  null|     BA|2147483647|00227|      WB|  2016-04-03| 2016-04-06|        3|\n",
      "|459666.0| 2016|     4|   135|   135|    ATL|  20547|      1|     GA|  20550|    51|      1|    1|20160403|    null| null|      G|      O|   null|      M|   1965|07012016|     M|  null|     BA|2147483647|00227|      WB|  2016-04-03| 2016-04-06|        3|\n",
      "|459667.0| 2016|     4|   135|   135|    ATL|  20547|      1|     GA|  20550|    43|      1|    1|20160403|    null| null|      G|      O|   null|      M|   1973|07012016|     M|  null|     DL|2147483647|00031|      WB|  2016-04-03| 2016-04-06|        3|\n",
      "|459668.0| 2016|     4|   135|   135|    ATL|  20547|      1|     GA|  20550|    37|      1|    1|20160403|    null| null|      G|      O|   null|      M|   1979|07012016|     F|  null|     DL|2147483647|00029|      WB|  2016-04-03| 2016-04-06|        3|\n",
      "|459669.0| 2016|     4|   135|   135|    ATL|  20547|      1|     GA|  20550|    33|      1|    1|20160403|    null| null|      G|      O|   null|      M|   1983|07012016|     F|  null|     VS|2147483647|00109|      WB|  2016-04-03| 2016-04-06|        3|\n",
      "|459670.0| 2016|     4|   135|   135|    ATL|  20547|      1|     GA|  20550|    50|      1|    1|20160403|    null| null|      G|      O|   null|      M|   1966|07012016|     M|  null|     DL|2147483647|00031|      WB|  2016-04-03| 2016-04-06|        3|\n",
      "+--------+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+----------+-----+--------+------------+-----------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_time.withColumn(\"diff_days\", F.datediff(\"depart_date\", \"arrival_date\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## map i94addrl - state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def map_col(df, map_col_name, df_col_name, new_col_name):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : spark dataframe\n",
    "        The file containing the df_col_name to be used for mapping.\n",
    "    map_col_name : str\n",
    "        The column name of the mapping file.\n",
    "    df_col_name : str\n",
    "        The column name in the Spark dataframe to be used.\n",
    "    new_col_name : str\n",
    "        New column name of the mapping results.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    df_map = pd.read_csv(os.path.join(DATAFOLDER, f'{map_col_name}.csv'), quotechar=\"'\")\n",
    "    id_col = f'{map_col_name}_id'\n",
    "    dic_map = dict(zip(df_map[id_col], df_map[map_col_name]))\n",
    "    mapping_expr = F.create_map([F.lit(x) for x in chain(*dic_map.items())])\n",
    "    return df.withColumn(new_col_name, mapping_expr[F.col(df_col_name)])\n",
    "\n",
    "\n",
    "\n",
    "@udf\n",
    "def udf_city_name(city_full):\n",
    "    splt = str.split(city_full, ',')\n",
    "    if len(splt) == 0:\n",
    "        return ''\n",
    "    return splt[0].capitalize().strip()\n",
    "\n",
    "@udf\n",
    "def udf_state_short(city_full):\n",
    "    splt = str.split(city_full, ',')\n",
    "    if len(splt) < 2:\n",
    "        return ''\n",
    "    return splt[1].strip()\n",
    "\n",
    "@udf\n",
    "def udf_state_format(port_state):\n",
    "    return port_state.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|i94addr|\n",
      "+-------+\n",
      "|     CI|\n",
      "|     FT|\n",
      "|     SC|\n",
      "|     AZ|\n",
      "|     PU|\n",
      "|     UA|\n",
      "|     EA|\n",
      "|     NS|\n",
      "|     KI|\n",
      "|     PI|\n",
      "+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.select(\"i94addr\").distinct().show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO replace mapping in i94addrl to have names in captialize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df, map_col_name, df_col_name, new_col_name\n",
    "df_test = df_time\n",
    "#df_test = map_col(df_test, 'i94prtl', 'i94port', 'port')\n",
    "\n",
    "# city name of arrival port\n",
    "#df_test = df_test.withColumn(\"port_city\", udf_city_name(\"port\"))\n",
    "df_test = map_col(df_test, 'prtl_city', 'i94port', 'port_city')\n",
    "#df_test = df_test.withColumn(\"port_state_short\", udf_state_short(\"port\"))\n",
    "df_test = map_col(df_test, 'prtl_state', 'i94port', 'port_state_short')\n",
    "df_test = map_col(df_test, 'addrl', 'port_state_short', 'port_state')\n",
    "#df_test = df_test.withColumn('port_state_2', udf_state_format('port_state'))\n",
    "\n",
    "# country of entry, country of citizenship\n",
    "df_test = map_col(df_test, 'cntyl', 'i94port', 'state_cit')\n",
    "df_test = map_col(df_test, 'cntyl', 'i94port', 'state_res')\n",
    "#df_test = map_col(df_test, 'i94addrl', 'i94addr', 'port_state')\n",
    "df_test = map_col(df_test, 'visa', 'i94visa', 'visa')\n",
    "#df_test = df_test.withColumn(\"port_state\", udf_state_format(\"port_state\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is arrdate ever null?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+------------+-----------+---------+----------------+----------+---------+---------+----+\n",
      "|cicid|i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto|gender|insnum|airline|admnum|fltno|visatype|arrival_date|depart_date|port_city|port_state_short|port_state|state_cit|state_res|visa|\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+------------+-----------+---------+----------------+----------+---------+---------+----+\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+-------+------+------+-------+------+-----+--------+------------+-----------+---------+----------------+----------+---------+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.where(F.col(\"arrdate\").isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge with demographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#demographic = pd.read_csv(os.path.join(DATAFOLDER, 'us-cities-demographics.csv'), sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics = spark.read.format('csv').options(header='true', sep=';', inferSchema=True).\\\n",
    "    load(os.path.join(DATAFOLDER, 'us-cities-demographics.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "match airport with demographic table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up unique combinations of city and state in fact table\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset with unique combinations of city, state and \n",
    "# port_city, port_state, i94port (as key )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+----------+-----+--------+------------+-----------+---------+----------------+----------+---------+---------+--------+\n",
      "|   cicid|i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|    admnum|fltno|visatype|arrival_date|depart_date|port_city|port_state_short|port_state|state_cit|state_res|    visa|\n",
      "+--------+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+----------+-----+--------+------------+-----------+---------+----------------+----------+---------+---------+--------+\n",
      "|459651.0| 2016|     4|   135|   135|    ATL|  20547|      1|     FL|  20559|    54|      2|    1|20160403|    null| null|      O|      R|   null|      M|   1962|07012016|  null|  null|     VS|2147483647|00115|      WT|  2016-04-03| 2016-04-15|  Atlanta|              GA|   Georgia|     null|     null|Pleasure|\n",
      "|459652.0| 2016|     4|   135|   135|    ATL|  20547|      1|     FL|  20555|    74|      2|    1|20160403|    null| null|      T|      O|   null|      M|   1942|07012016|     F|  null|     VS| 674406485|  103|      WT|  2016-04-03| 2016-04-11|  Atlanta|              GA|   Georgia|     null|     null|Pleasure|\n",
      "|459653.0| 2016|     4|   135|   135|    ATL|  20547|      1|     FL|  20557|    44|      2|    1|20160403|    null| null|      T|      Q|   null|      M|   1972|10022016|     M|  null|     VS| 674948185|  109|      B2|  2016-04-03| 2016-04-13|  Atlanta|              GA|   Georgia|     null|     null|Pleasure|\n",
      "|459654.0| 2016|     4|   135|   135|    ATL|  20547|      1|      G|  20555|    38|      2|    1|20160403|    null| null|      O|      O|   null|      M|   1978|07012016|  null|  null|     VS|2147483647|00103|      WT|  2016-04-03| 2016-04-11|  Atlanta|              GA|   Georgia|     null|     null|Pleasure|\n",
      "|459655.0| 2016|     4|   135|   135|    ATL|  20547|      1|     GA|   null|    64|      2|    1|20160403|    null| null|      G|   null|   null|   null|   1952|07012016|     F|  null|     VS|2147483647|00103|      WT|  2016-04-03|       null|  Atlanta|              GA|   Georgia|     null|     null|Pleasure|\n",
      "+--------+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+----------+-----+--------+------------+-----------+---------+----------------+----------+---------+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_read_csv(spark, folder, filename, **kwargs):\n",
    "\n",
    "    return spark.read.format('csv').options(header='true', inferSchema=True, **kwargs).\\\n",
    "        load(os.path.join(folder, filename))\n",
    "\n",
    "def format_column_names(s):\n",
    "    s = s.casefold()\n",
    "    s = s.replace(' ', '_')\n",
    "    s = s.replace('-', '_')\n",
    "    return s\n",
    "\n",
    "def rename_columns(df):\n",
    "    old_names = df.schema.names\n",
    "    new_names = [format_column_names(s) for s in old_names]\n",
    "    df = reduce(lambda df, idx: \n",
    "                df.withColumnRenamed(old_names[idx], new_names[idx]), range(len(old_names)), df)\n",
    "    return df\n",
    "\n",
    "def create_demographic_table(spark, datafolder, outputfolder): \n",
    "    # select a subset of original table\n",
    "    #df_demo = df.select(['i94port']).dropDuplicates()\n",
    "    df_demo = spark_read_csv(spark, datafolder, 'prtl_city.csv')\n",
    "    #pd.read_csv(os.path.join(datafolder, 'prtl.csv'), quotechar=\"'\")\n",
    "    demographics = spark_read_csv(spark, datafolder, 'us-cities-demographics.csv', sep=';')\n",
    "    #pd.read_csv(os.path.join(datafolder, 'us-cities-demographics.csv'), quotechar=\"'\")\n",
    "    \n",
    "    # do the preprocessing, append columns\n",
    "    #df_demo = map_col(df_demo, 'prtl_city', 'prtl_city', 'port_city')\n",
    "    df_demo = df_demo.withColumnRenamed('prtl_city', 'port_city')\n",
    "    #df_test = df_test.withColumn(\"port_state_short\", udf_state_short(\"port\"))\n",
    "    df_demo = map_col(df_demo, 'prtl_state', 'prtl_city_id', 'port_state_short')\n",
    "    df_demo = map_col(df_demo, 'addrl', 'port_state_short', 'port_state')\n",
    "    \n",
    "    # we are deleting the ones, for which no states were found, since we only focus on the US here\n",
    "    # later expand world wide\n",
    "    df_demo = df_demo.dropna(subset=['port_state'])\n",
    "    df_demo = df_demo.join(demographics, (df_demo.port_city == demographics.City) & (df_demo.port_state_short == demographics['State Code']))\n",
    "    columns_to_drop = ['port_city', 'port_state', 'port_state_short']\n",
    "    df_demo = df_demo.drop(*columns_to_drop)\n",
    "    \n",
    "    # reformat column names\n",
    "    df_demo = rename_columns(df_demo)\n",
    "    df_demo.write.parquet(os.path.join(outputfolder, 'city_demographics.parquet'), 'overwrite')\n",
    "    return df_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo = create_demographic_table(spark, DATAFOLDER, OUTPUTFOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-------+\n",
      "|prtl_city_id|           city|       state|median_age|male_population|female_population|total_population|number_of_veterans|foreign_born|average_household_size|state_code|                race|  count|\n",
      "+------------+---------------+------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-------+\n",
      "|         TAC|         Tacoma|  Washington|      37.7|         100914|           107036|          207950|             19040|       31863|                  2.48|        WA|Black or African-...|  30914|\n",
      "|         BIL|       Billings|     Montana|      36.3|          52705|            57565|          110270|              9121|        3206|                   2.4|        MT|               White| 102522|\n",
      "|         PRO|     Providence|Rhode Island|      29.9|          89090|            90114|          179204|              4933|       53532|     2.719999999999999|        RI|Black or African-...|  30638|\n",
      "|         ROC|      Rochester|    New York|      31.4|         100135|           109673|          209808|              7288|       17735|                  2.36|        NY|Black or African-...|  93072|\n",
      "|         BUF|        Buffalo|    New York|      33.1|         124537|           133529|          258066|             11231|       24630|                  2.27|        NY|Black or African-...| 101254|\n",
      "|         RCM|       Richmond|    Virginia|      33.6|         104793|           115496|          220289|             12538|       15741|                  2.29|        VA|               White| 104568|\n",
      "|         SAA|      Santa Ana|  California|      30.8|         167503|           167920|          335423|              4735|      152999|                  4.58|        CA|               White| 163096|\n",
      "|         MIL|      Milwaukee|   Wisconsin|      31.6|         286315|           313839|          600154|             20615|       58321|                  2.51|        WI|Black or African-...| 248149|\n",
      "|         SAV|       Savannah|     Georgia|      30.3|          69389|            76295|          145684|              9717|       10355|    2.5700000000000003|        GA|               White|  57690|\n",
      "|         NWH|      New Haven| Connecticut|      29.9|          63765|            66545|          130310|              2567|       25871|                  2.48|        CT|               White|  56251|\n",
      "|         TUC|         Tucson|     Arizona|      33.6|         264893|           266781|          531674|             38182|       82220|                  2.45|        AZ|  Hispanic or Latino| 231025|\n",
      "|         OAK|        Oakland|  California|      35.7|         203827|           215451|          419278|             12159|      113896|                  2.56|        CA|               White| 171599|\n",
      "|         LOS|    Los Angeles|  California|      35.0|        1958998|          2012898|         3971896|             85417|     1485425|                  2.86|        CA|               White|2177650|\n",
      "|         DSM|     Des Moines|        Iowa|      34.5|         103726|           106591|          210317|             11780|       23857|                  2.41|        IA|  Hispanic or Latino|  25306|\n",
      "|         TAC|         Tacoma|  Washington|      37.7|         100914|           107036|          207950|             19040|       31863|                  2.48|        WA|American Indian a...|   5549|\n",
      "|         MIL|      Milwaukee|   Wisconsin|      31.6|         286315|           313839|          600154|             20615|       58321|                  2.51|        WI|  Hispanic or Latino| 110335|\n",
      "|         WPB|West Palm Beach|     Florida|      39.6|          49262|            57520|          106782|              4917|       30675|    2.5300000000000002|        FL|  Hispanic or Latino|  26132|\n",
      "|         BYO|        Bayonne|  New Jersey|      39.7|          32705|            33598|           66303|              2225|       21899|                  2.62|        NJ|Black or African-...|   7581|\n",
      "|         NWH|      New Haven| Connecticut|      29.9|          63765|            66545|          130310|              2567|       25871|                  2.48|        CT|Black or African-...|  43356|\n",
      "|         ATL|        Atlanta|     Georgia|      33.8|         223960|           239915|          463875|             18572|       32016|                  2.15|        GA|  Hispanic or Latino|  18653|\n",
      "+------------+---------------+------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demo.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create single tables for everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## country table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spark_read_csv(folder, filename, **kwargs):\n",
    "\n",
    "    return spark.read.format('csv').options(header='true', inferSchema=True, **kwargs).\\\n",
    "        load(os.path.join(folder, filename))\n",
    "\n",
    "def csv_to_parquet(datafolder, outputfolder, csv_name, table_name): \n",
    "    df = spark_read_csv(datafolder, f'{csv_name}.csv', sep=',', quotechar=\"'\")\n",
    "    df = df.withColumnRenamed('value', 'id')\n",
    "    df.write.parquet(os.path.join(outputfolder, f'{table_name}.parquet'), 'overwrite')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntyl = csv_to_parquet(DATAFOLDER, OUTPUTFOLDER, 'i94cntyl', 'country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_col_name = 'prtl_state'\n",
    "df_map = spark_read_csv(spark, datafolder, f'{map_col_name}.csv')\n",
    "#pd.read_csv(os.path.join(DATAFOLDER, f'{map_col_name}.csv'), quotechar=\"'\")\n",
    "df_map = df_map.toPandas()\n",
    "id_col = f'{map_col_name}_id'\n",
    "dic_map = dict(zip(df_map[id_col], df_map[map_col_name]))\n",
    "mapping_expr = F.create_map([F.lit(x) for x in chain(*dic_map.items())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Column is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-ec92385a84c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/lib/python3.7/site-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Column is not iterable\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# string methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Column is not iterable"
     ]
    }
   ],
   "source": [
    "list(df_map[id_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model table i94model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "transport = csv_to_parquet(DATAFOLDER, OUTPUTFOLDER, 'i94model', 'transport')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------+\n",
      "| id|      i94model|\n",
      "+---+--------------+\n",
      "|  1|         'Air'|\n",
      "|  2|         'Sea'|\n",
      "|  3|        'Land'|\n",
      "|  9|'Not reported'|\n",
      "+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transport.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visa table i94visa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "visa = csv_to_parquet(DATAFOLDER, OUTPUTFOLDER, 'i94visa', 'visa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| id| i94visa|\n",
      "+---+--------+\n",
      "|  1|Business|\n",
      "|  2|Pleasure|\n",
      "|  3| Student|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visa.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## global annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "# also aggregate by year\n",
    "folder = os.path.join(DATAFOLDER, 'climate-change-earth-surface-temperature-data')\n",
    "global_annual = csv_to_parquet(folder, OUTPUTFOLDER, 'GlobalTemperatures', 'visa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_full_time_table(outputfolder, daysafter=36525):\n",
    "\n",
    "    days_till_2060 = range(daysafter)\n",
    "    all_dates = [(t,) for t in days_till_2060]\n",
    "    \n",
    "    t_schema = T.StructType([T.StructField('i94_date', T.IntegerType())])\n",
    "    timeframe = spark.createDataFrame(all_dates, t_schema)\n",
    "    timeframe = timeframe.withColumn(\"dt_date\", F.expr(\"date_add(to_date('1960-01-01'), i94_date)\"))\n",
    "    timeframe = timeframe.select('i94_date', 'dt_date',\n",
    "                    F.year('dt_date').alias('year'),\n",
    "                    F.month('dt_date').alias('month'),\n",
    "                    F.dayofmonth('dt_date').alias('day'),\n",
    "                    F.dayofweek('dt_date').alias('weekday'))\n",
    "    \n",
    "    timeframe.write.partitionBy('year', 'month').parquet(os.path.join(outputfolder, 'dates.parquet'), 'overwrite')\n",
    "    return timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeframe = create_full_time_table(OUTPUTFOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i94_date', 'int'),\n",
       " ('dt_date', 'date'),\n",
       " ('year', 'int'),\n",
       " ('month', 'int'),\n",
       " ('day', 'int'),\n",
       " ('weekday', 'int')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timeframe.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----+-----+---+-------+\n",
      "|i94_date|   dt_date|year|month|day|weekday|\n",
      "+--------+----------+----+-----+---+-------+\n",
      "|       0|1960-01-01|1960|    1|  1|      6|\n",
      "|       1|1960-01-02|1960|    1|  2|      7|\n",
      "|       2|1960-01-03|1960|    1|  3|      1|\n",
      "|       3|1960-01-04|1960|    1|  4|      2|\n",
      "|       4|1960-01-05|1960|    1|  5|      3|\n",
      "|       5|1960-01-06|1960|    1|  6|      4|\n",
      "|       6|1960-01-07|1960|    1|  7|      5|\n",
      "|       7|1960-01-08|1960|    1|  8|      6|\n",
      "|       8|1960-01-09|1960|    1|  9|      7|\n",
      "|       9|1960-01-10|1960|    1| 10|      1|\n",
      "|      10|1960-01-11|1960|    1| 11|      2|\n",
      "|      11|1960-01-12|1960|    1| 12|      3|\n",
      "|      12|1960-01-13|1960|    1| 13|      4|\n",
      "|      13|1960-01-14|1960|    1| 14|      5|\n",
      "|      14|1960-01-15|1960|    1| 15|      6|\n",
      "|      15|1960-01-16|1960|    1| 16|      7|\n",
      "|      16|1960-01-17|1960|    1| 17|      1|\n",
      "|      17|1960-01-18|1960|    1| 18|      2|\n",
      "|      18|1960-01-19|1960|    1| 19|      3|\n",
      "|      19|1960-01-20|1960|    1| 20|      4|\n",
      "+--------+----------+----+-----+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timeframe.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_full_time_table(outputfolder, daysafter=36525):\n",
    "\n",
    "    future_days = range(daysafter)\n",
    "    all_dates = [(t,) for t in future_days]\n",
    "    \n",
    "    t_schema = T.StructType([T.StructField('i_date', T.IntegerType())])\n",
    "    timeframe = spark.createDataFrame(all_dates, t_schema)\n",
    "    timeframe = timeframe.withColumn(\"dt_date\", F.expr(\"date_add(to_date('1960-01-01'), i_date)\"))\n",
    "    timeframe = timeframe.select('i_date', 'dt_date',\n",
    "                    F.year('dt_date').alias('year'),\n",
    "                    F.month('dt_date').alias('month'),\n",
    "                    F.dayofmonth('dt_date').alias('day'),\n",
    "                    F.dayofweek('dt_date').alias('weekday'))\n",
    "    \n",
    "    timeframe.write.partitionBy('year', 'month').parquet(os.path.join(outputfolder, 'dates.parquet'), 'overwrite')\n",
    "    return timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = create_full_time_table(OUTPUTFOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(i_date=0, dt_date=datetime.date(1960, 1, 1), year=1960, month=1, day=1, weekday=6)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Climate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO apply rolling window function on average of last 10 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = os.path.join(DATAFOLDER, 'climate-change-earth-surface-temperature-data')\n",
    "filename = 'GlobalLandTemperaturesByCountry.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate = spark_read_csv(folder, filename, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert timestamp to date\n",
    "already done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dt', 'timestamp'),\n",
       " ('AverageTemperature', 'double'),\n",
       " ('AverageTemperatureUncertainty', 'double'),\n",
       " ('Country', 'string')]"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climate.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate = climate.withColumn('dt', F.to_date(F.col('dt')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "merge with country id from country table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(1743, 11, 1)"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climate.agg({\"dt\": \"min\"}).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2013, 9, 1)"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climate.agg({\"dt\": \"max\"}).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "country = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(OUTPUTFOLDER, 'country.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate = climate.join(country, on=country['i94cntyl'] == climate['Country'], how='leftouter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate = climate.drop('i94cntyl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_climate_country(folder, filename, outputfolder):\n",
    "    climate = spark_read_csv(folder, filename, sep=',')\n",
    "    climate = climate.withColumn('dt', F.to_date(F.col('dt')))\n",
    "    country = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(outputfolder, 'country.parquet'))\n",
    "    climate = climate.join(country, on=country['i94cntyl'] == climate['Country'], how='leftouter')\n",
    "    climate = climate.drop('i94cntyl')\n",
    "    climate = rename_columns(climate)\n",
    "    climate = climate.withColumn('year', F.year('dt').alias('year'))\n",
    "    climate = climate.withColumnRenamed('id', 'country_id').\\\n",
    "        withColumnRenamed('averagetemperatureuncertainty', 'avg_uncertainty').\\\n",
    "        withColumnRenamed('averagetemperature', 'avg_temperature')\n",
    "    climate.write.partitionBy('year', 'country').parquet(os.path.join(outputfolder, 'climate_country.parquet'), 'overwrite')\n",
    "    return climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = os.path.join(DATAFOLDER, 'climate-change-earth-surface-temperature-data')\n",
    "filename = 'GlobalLandTemperaturesByCountry.csv'\n",
    "climate_country = generate_climate_country(folder, filename, OUTPUTFOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_country = climate_country.withColumnRenamed('id', 'country_id').\\\n",
    "    withColumnRenamed('averagetemperatureuncertainty', 'avg_uncertainty')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aggregate annual average temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual = climate.withColumn('year', F.year('dt').alias('year'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----+------------------+\n",
      "|   country|country_id|year|   avg_temperature|\n",
      "+----------+----------+----+------------------+\n",
      "|   Albania|       101|1838|11.598333333333334|\n",
      "|   Algeria|       316|1918|          22.41125|\n",
      "|   Andorra|       102|1855|10.281666666666666|\n",
      "|   Andorra|       102|1903|10.768500000000001|\n",
      "|   Andorra|       102|1933|11.168999999999999|\n",
      "|  Anguilla|       529|1862|          26.02125|\n",
      "|  Anguilla|       529|1924|26.494000000000003|\n",
      "|  Anguilla|       529|1969|27.049250000000004|\n",
      "|   Armenia|       151|1858|7.9815000000000005|\n",
      "|   Armenia|       151|1926| 8.654083333333332|\n",
      "|   Armenia|       151|1995| 9.892166666666668|\n",
      "|     Aruba|       532|1937|28.008666666666667|\n",
      "|Azerbaijan|       152|1852|            10.991|\n",
      "|Azerbaijan|       152|1856|10.825583333333334|\n",
      "|   Bahrain|       298|1848|25.192666666666668|\n",
      "|  Barbados|       513|1962|            26.849|\n",
      "|   Belarus|       153|1998| 6.528333333333333|\n",
      "|    Bhutan|       242|1871|11.630416666666667|\n",
      "|    Bhutan|       242|1907|11.554749999999999|\n",
      "|    Bhutan|       242|1929|          11.64875|\n",
      "+----------+----------+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "climate_country.groupby([F.col('country'), F.col('country_id'), F.col('year')]).agg(F.avg('averagetemperature').alias('avg_temperature')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "annual = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(OUTPUTFOLDER, 'climate_country.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_annual_temp_table(outputfolder):\n",
    "    annual = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(OUTPUTFOLDER, 'climate_country.parquet'))\n",
    "    annual.groupby([F.col('country'), F.col('country_id'), F.col('year')]).agg(F.avg('avg_temperature').alias('avg_temperature'))\n",
    "    annual.write.partitionBy('country').parquet(os.path.join(outputfolder, 'annual_climate_country.parquet'), 'overwrite')\n",
    "    return annual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`country_id`' given input columns: [Country, id, averagetemperatureuncertainty, year, dt, averagetemperature];;\\n'Aggregate [country#15911, 'country_id, year#15910], [country#15911, 'country_id, year#15910, avg(AverageTemperature#15907) AS avg(AverageTemperature)#15925]\\n+- Relation[dt#15906,averagetemperature#15907,averagetemperatureuncertainty#15908,id#15909,year#15910,Country#15911] parquet\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o82809.agg.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`country_id`' given input columns: [Country, id, averagetemperatureuncertainty, year, dt, averagetemperature];;\n'Aggregate [country#15911, 'country_id, year#15910], [country#15911, 'country_id, year#15910, avg(AverageTemperature#15907) AS avg(AverageTemperature)#15925]\n+- Relation[dt#15906,averagetemperature#15907,averagetemperatureuncertainty#15908,id#15909,year#15910,Country#15911] parquet\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:279)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:69)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:186)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:58)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:56)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)\n\tat org.apache.spark.sql.RelationalGroupedDataset.toDF(RelationalGroupedDataset.scala:65)\n\tat org.apache.spark.sql.RelationalGroupedDataset.agg(RelationalGroupedDataset.scala:224)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:745)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-463-0ea2ac5dff86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mannual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mergeSchema\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOUTPUTFOLDER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'climate_country.parquet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mannual\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'country'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'country_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'AverageTemperature'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda/lib/python3.7/site-packages/pyspark/sql/group.py\u001b[0m in \u001b[0;36magg\u001b[0;34m(self, *exprs)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexprs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"all exprs should be Column\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             jdf = self._jgd.agg(exprs[0]._jc,\n\u001b[0;32m--> 115\u001b[0;31m                                 _to_seq(self.sql_ctx._sc, [c._jc for c in exprs[1:]]))\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`country_id`' given input columns: [Country, id, averagetemperatureuncertainty, year, dt, averagetemperature];;\\n'Aggregate [country#15911, 'country_id, year#15910], [country#15911, 'country_id, year#15910, avg(AverageTemperature#15907) AS avg(AverageTemperature)#15925]\\n+- Relation[dt#15906,averagetemperature#15907,averagetemperatureuncertainty#15908,id#15909,year#15910,Country#15911] parquet\\n\""
     ]
    }
   ],
   "source": [
    "annual = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(OUTPUTFOLDER, 'climate_country.parquet'))\n",
    "annual.groupby([F.col('country'), F.col('country_id'), F.col('year')]).agg(F.avg('averagetemperature').alias('avg_temperature'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo when storing partition by country and year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asylum Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/dhs/refugee-report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fact Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Calculate length of stay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(cicid=459651.0, i94yr=2016, i94mon=4, i94cit=135, i94res=135, i94port='ATL', arrdate=20547, i94mode=1, i94addr='FL', depdate=20559, i94bir=54, i94visa=2, count=1, dtadfile='20160403', visapost=None, occup=None, entdepa='O', entdepd='R', entdepu=None, matflag='M', biryear=1962, dtaddto='07012016', gender=None, insnum=None, airline='VS', admnum=2147483647, fltno='00115', visatype='WT', arrival_date=datetime.date(2016, 4, 3), depart_date=datetime.date(2016, 4, 15), port_city='Atlanta', port_state_short='GA', port_state='Georgia', state_cit='United Kingdom', state_res='United Kingdom', visa='Pleasure')"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|   dt_date|\n",
      "+----------+\n",
      "|2016-04-29|\n",
      "|2016-04-29|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "timeframe.filter(\"i94_date = 20573\").select('dt_date').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact = df_test.withColumn(\"arrival_dt\", F.expr(\"date_add(to_date('1960-01-01'), arrdate)\"))\n",
    "df_fact = df_fact.withColumn(\"depart_dt\", F.expr(\"date_add(to_date('1960-01-01'), depdate)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_columns = ['cicid', 'i94yr', 'i94mon', 'arrival_dt', 'arrdate', 'depdate', 'i94cit', 'i94res', 'i94port', 'i94mode', 'i94addr', 'i94bir',\n",
    "               'i94visa', 'visatype', 'biryear', 'gender', 'airline', 'fltno', 'length_stay']\n",
    "\n",
    "#maybe = ['entdepa', 'entdepd', 'matflag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact = df_fact.withColumn(\"length_stay\", F.datediff(\"depart_date\", \"arrival_date\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------+----------+-------+-------+------+------+-------+-------+-------+------+-------+--------+-------+------+-------+-----+-----------+\n",
      "|   cicid|i94yr|i94mon|arrival_dt|arrdate|depdate|i94cit|i94res|i94port|i94mode|i94addr|i94bir|i94visa|visatype|biryear|gender|airline|fltno|length_stay|\n",
      "+--------+-----+------+----------+-------+-------+------+------+-------+-------+-------+------+-------+--------+-------+------+-------+-----+-----------+\n",
      "|459651.0| 2016|     4|2016-04-03|  20547|  20559|   135|   135|    ATL|      1|     FL|    54|      2|      WT|   1962|  null|     VS|00115|         12|\n",
      "|459652.0| 2016|     4|2016-04-03|  20547|  20555|   135|   135|    ATL|      1|     FL|    74|      2|      WT|   1942|     F|     VS|  103|          8|\n",
      "|459653.0| 2016|     4|2016-04-03|  20547|  20557|   135|   135|    ATL|      1|     FL|    44|      2|      B2|   1972|     M|     VS|  109|         10|\n",
      "|459654.0| 2016|     4|2016-04-03|  20547|  20555|   135|   135|    ATL|      1|      G|    38|      2|      WT|   1978|  null|     VS|00103|          8|\n",
      "|459655.0| 2016|     4|2016-04-03|  20547|   null|   135|   135|    ATL|      1|     GA|    64|      2|      WT|   1952|     F|     VS|00103|       null|\n",
      "|459656.0| 2016|     4|2016-04-03|  20547|   null|   135|   135|    ATL|      1|     GA|    63|      2|      WT|   1953|     M|     BA|00227|       null|\n",
      "|459657.0| 2016|     4|2016-04-03|  20547|  20548|   135|   135|    ATL|      1|     GA|    44|      1|      WB|   1972|     M|     BA|00227|          1|\n",
      "|459658.0| 2016|     4|2016-04-03|  20547|  20548|   135|   135|    ATL|      1|     GA|    39|      1|      WB|   1977|     M|     BA|00227|          1|\n",
      "|459659.0| 2016|     4|2016-04-03|  20547|  20573|   135|   135|    ATL|      1|     GA|    84|      2|      WT|   1932|     M|     BA|00227|         26|\n",
      "|459660.0| 2016|     4|2016-04-03|  20547|  20555|   135|   135|    ATL|      1|     GA|    55|      2|      WT|   1961|     M|     BA|00227|          8|\n",
      "|459661.0| 2016|     4|2016-04-03|  20547|  20555|   135|   135|    ATL|      1|     GA|    54|      2|      WT|   1962|     M|     BA|00227|          8|\n",
      "|459662.0| 2016|     4|2016-04-03|  20547|  20548|   135|   135|    ATL|      1|     GA|    36|      2|      WT|   1980|     M|     DL|00349|          1|\n",
      "|459663.0| 2016|     4|2016-04-03|  20547|  20549|   135|   135|    ATL|      1|     GA|    71|      1|      WB|   1945|     M|     DL|00031|          2|\n",
      "|459664.0| 2016|     4|2016-04-03|  20547|  20549|   135|   135|    ATL|      1|     GA|    39|      2|      WT|   1977|     M|     VS|00109|          2|\n",
      "|459665.0| 2016|     4|2016-04-03|  20547|  20550|   135|   135|    ATL|      1|     GA|    54|      1|      WB|   1962|     M|     BA|00227|          3|\n",
      "|459666.0| 2016|     4|2016-04-03|  20547|  20550|   135|   135|    ATL|      1|     GA|    51|      1|      WB|   1965|     M|     BA|00227|          3|\n",
      "|459667.0| 2016|     4|2016-04-03|  20547|  20550|   135|   135|    ATL|      1|     GA|    43|      1|      WB|   1973|     M|     DL|00031|          3|\n",
      "|459668.0| 2016|     4|2016-04-03|  20547|  20550|   135|   135|    ATL|      1|     GA|    37|      1|      WB|   1979|     F|     DL|00029|          3|\n",
      "|459669.0| 2016|     4|2016-04-03|  20547|  20550|   135|   135|    ATL|      1|     GA|    33|      1|      WB|   1983|     F|     VS|00109|          3|\n",
      "|459670.0| 2016|     4|2016-04-03|  20547|  20550|   135|   135|    ATL|      1|     GA|    50|      1|      WB|   1966|     M|     DL|00031|          3|\n",
      "+--------+-----+------+----------+-------+-------+------+------+-------+-------+-------+------+-------+--------+-------+------+-------+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fact.select(keep_columns).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cicid', 'double'),\n",
       " ('i94yr', 'int'),\n",
       " ('i94mon', 'int'),\n",
       " ('i94cit', 'int'),\n",
       " ('i94res', 'int'),\n",
       " ('i94port', 'string'),\n",
       " ('arrdate', 'int'),\n",
       " ('i94mode', 'int'),\n",
       " ('i94addr', 'string'),\n",
       " ('depdate', 'int'),\n",
       " ('i94bir', 'int'),\n",
       " ('i94visa', 'int'),\n",
       " ('count', 'int'),\n",
       " ('dtadfile', 'string'),\n",
       " ('visapost', 'string'),\n",
       " ('occup', 'string'),\n",
       " ('entdepa', 'string'),\n",
       " ('entdepd', 'string'),\n",
       " ('entdepu', 'string'),\n",
       " ('matflag', 'string'),\n",
       " ('biryear', 'int'),\n",
       " ('dtaddto', 'string'),\n",
       " ('gender', 'string'),\n",
       " ('insnum', 'string'),\n",
       " ('airline', 'string'),\n",
       " ('admnum', 'int'),\n",
       " ('fltno', 'string'),\n",
       " ('visatype', 'string'),\n",
       " ('arrival_date', 'date'),\n",
       " ('depart_date', 'date'),\n",
       " ('port_city', 'string'),\n",
       " ('port_state_short', 'string'),\n",
       " ('port_state', 'string'),\n",
       " ('state_cit', 'string'),\n",
       " ('state_res', 'string'),\n",
       " ('visa', 'string'),\n",
       " ('arrival_dt', 'date'),\n",
       " ('depart_dt', 'date'),\n",
       " ('length_stay', 'int')]"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fact.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_facts_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test.join(timeframe, on=df_test['arrdate']==timeframe['i94_date'], how='leftouter').show() \n",
    "# too complicated\n",
    "#df_test.join(timeframe, on=df_test['arrdate']==timeframe['i94_date'], how='left').dropDuplicates().select(timeframe['dt_date']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "439428"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.withColumn(\"length_stay\", F.datediff(\"depart_date\", \"arrival_date\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# TODO\n",
    "\n",
    "- copy my csv files to S3 manually\n",
    "with a DAG\n",
    "- write some csvs to redshift \n",
    "- copy the stuff to redshift\n",
    "- preprocessed tables as parquet files?\n",
    "- maybe split it up into two DAGs\n",
    "-- one does the intialization of the dimension tables, which won't change that often\n",
    "-- the other one does the updating of the fact table\n",
    "\n",
    "- add IATA code to city ?\n",
    "\n",
    "- or maybe the nodes can be confiured to only run once?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. csv von S3 -> redshift\n",
    "2. parquet von S3 -> Spark -> redshift, maybe like this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://sonra.io/2018/01/01/using-apache-airflow-to-build-a-data-pipeline-on-aws/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COPY listing\n",
    "FROM 's3://mybucket/data/listings/parquet/'\n",
    "IAM_ROLE 'arn:aws:iam::0123456789012:role/MyRedshiftRole'\n",
    "FORMAT AS PARQUET;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If nothing works use Docker\n",
    "https://towardsdatascience.com/getting-started-with-airflow-using-docker-cd8b44dbff98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional databases\n",
    "- https://www.kaggle.com/open-flights/flight-route-database\n",
    "- https://www.kaggle.com/dhs/refugee-report\n",
    "- https://www.dhs.gov/immigration-statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/christian/Data/udacity_capstone/output'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUTFOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_country = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(OUTPUTFOLDER, 'country.parquet')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cntyl_id', 'cntyl']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_country.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp_country = spark.read.option(\"mergeSchema\", \"true\").parquet(os.path.join(OUTPUTFOLDER, 'temperature_country.parquet')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dt',\n",
       " 'avg_temperature',\n",
       " 'avg_uncertainty',\n",
       " 'cntyl_id',\n",
       " 'cntyl',\n",
       " 'year',\n",
       " 'country']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp_country.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
